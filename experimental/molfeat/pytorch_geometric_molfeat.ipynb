{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) 2023 Graphcore Ltd. All rights reserved."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Molecular Prediction on IPUs using MolFeat \n",
    "## Pytorch Geometric with Lipophilicity and QM9 Datasets\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction: MolFeat on the IPU\n",
    "\n",
    "The popular [MolFeat Library](https://molfeat.datamol.io) provides as open source hub of pre-trained featurizers for molecules to deploy directly into ML workflows. \n",
    "In this notebook we'll show exactly how to do that using the Graphcore IPU to train a PyTorch Geometric Graph Neural Network (GNN) on the `Lipophilicity` dataset from MoleculeNet to predict octanol/water distribution coefficients. \n",
    "Then we show how this can be extended to any regression task on a new dataset.\n",
    "\n",
    "\n",
    "\n",
    "### Summary table\n",
    "|  Domain | Tasks | Model | Datasets | Workflow |   Number of IPUs   | Execution time |\n",
    "|---------|-------|-------|----------|----------|--------------|--------------|\n",
    "|   Molecules   |  Regression  | GINE | Lipophilicity / QM9 | Training, evaluation, inference | recommended: 4x (min: 1x) | 5mn    |\n",
    "\n",
    "### Learning outcomes\n",
    "In this demo you will learn how to:\n",
    "- Predict molecular properties on molecules using a MolFeat featurizer and aPyTorchGeometric GNN on the IPU\n",
    "- How to build an inference workflow for single molecule predictions\n",
    "\n",
    "\n",
    "### Links to other resources\n",
    "For more information about MolFeat check out their [documentation](https://molfeat.datamol.io), and about the datasets used in this notebook look at [MoleculeNet](https://moleculenet.org/datasets-1). \n",
    "For this notebook a familiarity with GNNs and PyTorch Geometric is assumed, to refresh on any details the tutorials on using PyTorch Geometric on the IPU can be found [here](https://github.com/graphcore/Gradient-Pytorch-Geometric/tree/main/learning-pytorch-geometric-on-ipus).\n",
    "\n",
    "\n",
    "[![Join our Slack\n",
    "Community](https://img.shields.io/badge/Slack-Join%20Graphcore's%20Community-blue?style=flat-square&logo=slack)](https://www.graphcore.ai/join-community)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "\n",
    "This notebook is currently only supported for Graphcore's SDK 3.2.1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "sdk_version = os.getenv(\"SDK_VERSION\", \"\")\n",
    "if sdk_version == \"3.2.1\":\n",
    "    print(\"SDK check passed.\")\n",
    "else:\n",
    "    raise Exception(\n",
    "        f\"The current SDK version {sdk_version} is incompatible with this notebook. We recommend you relaunch this notebook with the SDK 3.2.1.\"\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running on Paperspace\n",
    "\n",
    "The Paperspace environment lets you run this notebook with no set up. To improve your experience we preload datasets and pre-install packages, this can take a few minutes, if you experience errors immediately after starting a session please try restarting the kernel before contacting support. If a problem persists or you want to give us feedback on the content of this notebook, please reach out to through our community of developers using our [slack channel](https://www.graphcore.ai/join-community) or raise a [GitHub issue](https://github.com/graphcore/examples).\n",
    "\n",
    "Requirements:\n",
    "\n",
    "* Python packages installed with `pip install -r ./requirements.txt`\n",
    "\n",
    "In order to improve usability and support for future users, Graphcore would like to collect information about the\n",
    "applications and code being run in this notebook. The following information will be anonymised before being sent to Graphcore:\n",
    "\n",
    "- User progression through the notebook\n",
    "- Notebook details: number of cells, code being run and the output of the cells\n",
    "- Environment details\n",
    "\n",
    "You can disable logging at any time by running `%unload_ext graphcore_cloud_tools.notebook_logging.gc_logger` from any cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install  -r ./requirements.txt\n",
    "%load_ext graphcore_cloud_tools.notebook_logging.gc_logger"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Problem...\n",
    "\n",
    "Let's start by posing a toy problem. Imagine we have a dataset of molecules that we know something about, in this case about the Lipophilicity of the molecules (the ability of a molecule to dissolve in non-water based solvents), but it could be any property we wish. This dataset has been made from experimental results which are very expensive and time consuming to produce or extend. \n",
    "\n",
    "So like any good scientist, we wonder can we take the data we have and create a model to describe the physical results that we can use to extend to new molecules where experiments haven't been carried about, and yield reliable results? \n",
    "\n",
    "Below we can see an example molecule from the dataset, Clopidogrel, represented as a SMILES string and visualised in 3D. Looking at the table we can see the expected result.\n",
    "This is the target we are aiming to produce, and by the end of this notebook we will have a model to fill in the rest of the table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import report_molecule_regression\n",
    "\n",
    "report_molecule_regression(\n",
    "    \"CHEMBL1940306\", 1.51, None, r\"CS(=O)(=O)c1ccc(Oc2ccc(cc2)C#C[C@]3(O)CN4CCC3CC4)cc1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make imported python modules automatically reload when the files are changed\n",
    "# needs to be before the first import.\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm\n",
    "from IPython.display import clear_output\n",
    "import ipywidgets as widgets\n",
    "\n",
    "import poptorch\n",
    "from poptorch_geometric.dataloader import CustomFixedSizeDataLoader\n",
    "from utils import (\n",
    "    plot_smoothed_loss,\n",
    "    report_molecule_regression,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyG integration"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen in the [molfeat integration tutorial](https://github.com/datamol-io/molfeat/blob/main/docs/tutorials/pyg_integration.ipynb), molfeat integrates easily with the PyTorch ecosystem. In this tutorial, we will demonstrate how you can integrate molfeat with [PyG](https://pytorch-geometric.readthedocs.io/en/latest/) for training SOTA GNNs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from molfeat.trans.graph.adj import PYGGraphTransformer\n",
    "from molfeat.calc.atom import AtomCalculator\n",
    "from molfeat.calc.bond import EdgeMatCalculator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Featurizer\n",
    "\n",
    "The key advantage of using MolFeat as part of the GNN pipeline is the pre-trained featurizers. These featurizers will allow us to take a molecule and build a set of node and edge features that is already discriminative and describes the relatonships between atoms and bonds. \n",
    "This is because the pre-trained featurizers have been trained on large quantities of data from a range of domains in order to capture the basic underlying physical relationships between atoms that is transferable between downstream tasks. \n",
    "\n",
    "We first start by defining our featurizer. We will use the `PYGGraphTransformer` from molfeat with atom and bond featurizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurizer = PYGGraphTransformer(\n",
    "    atom_featurizer=AtomCalculator(), bond_featurizer=EdgeMatCalculator()\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "For the dataset, we will use the `Lipophilicity` dataset (LogD) from MoleculeNet, which contains experimental results of octanol/water distribution coefficient at pH=7.4.\n",
    "This contains a list of molecules given by their CMPD_CHEMBLID, the SMILES string describing the molecular structure, and the expected result. \n",
    "\n",
    "We can look at the data in the dataframe in the output below, but take note of the length of the dataset. \n",
    "This is partly where the MolFeat Featurizer starts to make a lot of sense, with only 4200 molecules in the dataset we need to maximise the learning on downstream tasks to get good performance, and needing to learn meaningful chemical features would make this a significantly harder problem.\n",
    "This featurization can be offloaded to `MolFeat` as core features around how molecules are constructed will be consistent and transferable between tasks. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    \"https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/Lipophilicity.csv\"\n",
    ")\n",
    "print(df.head())\n",
    "print(f\"Length of dataset: {len(df)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try and visualise the dataset a bit better, we can plot a 3D representation of the molecules from our dataset and scan through with the slider to see how they look and the expected target values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mol_id = 10  # SET THIS VALUE AND RE-RUN THE CELL TO EXPLORE THE DATASET\n",
    "\n",
    "view = report_molecule_regression(\n",
    "    df.CMPD_CHEMBLID.values[mol_id],\n",
    "    df.exp.values[mol_id],\n",
    "    None,\n",
    "    df.smiles.values[mol_id],\n",
    ")\n",
    "view.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extension:** To use a different dataset, just load the dataset from csv and note the column headings from your dataframe. You will likely need to change the name `CMPD_CHEMBLID`/`mol_id` and `exp`/`gap` headings which correspond to the `Lipophilicity` and `QM9` datasets respectively. In the cell below you can make sure you make the right changes. Then when you load the dataset make sure the same changes are made to ensure the correct columns are read from the dataframe. Be careful in the Demo section at the end - you will need to make the same changes there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively we can use the QM9 dataset\n",
    "# df = pd.read_csv(\"https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/qm9.csv\")\n",
    "# print(df.head())\n",
    "# print(f\"Length of dataset: {len(df)}\")\n",
    "# mol_id = 10  # Pick a value between 0 and 2050 for the BBBP dataset to explore\n",
    "# report_molecule_regression(df.mol_id.values[mol_id], df.gap.values[mol_id], None, df.smiles.values[mol_id])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since training a network with PyTorch requires defining a dataset and dataloader, we can define our custom dataset that will take \n",
    "**(1)** the SMILES string of the molecule,\n",
    "**(2)** the LogD measurement, and \n",
    "**(3)** our molfeat transformer\n",
    "as input to generate the data point we need for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch_geometric.utils import degree\n",
    "\n",
    "\n",
    "class DTset(Dataset):\n",
    "    def __init__(self, smiles, y, featurizer):\n",
    "        super().__init__()\n",
    "        self.smiles = smiles\n",
    "        self.featurizer = featurizer\n",
    "        self.featurizer.auto_self_loop()\n",
    "        self.y = torch.tensor(y.astype(\"float16\")).unsqueeze(-1).float()\n",
    "        self.transformed_mols = self.featurizer(smiles)\n",
    "        self._degrees = None\n",
    "\n",
    "    @property\n",
    "    def num_atom_features(self):\n",
    "        return self.featurizer.atom_dim\n",
    "\n",
    "    @property\n",
    "    def num_output(self):\n",
    "        return self.y.shape[-1]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.transformed_mols)\n",
    "\n",
    "    @property\n",
    "    def num_bond_features(self):\n",
    "        return self.featurizer.bond_dim\n",
    "\n",
    "    @property\n",
    "    def degree(self):\n",
    "        if self._degrees is None:\n",
    "            max_degree = -1\n",
    "            for data in self.transformed_mols:\n",
    "                d = degree(\n",
    "                    data.edge_index[1], num_nodes=data.num_nodes, dtype=torch.long\n",
    "                )\n",
    "                max_degree = max(max_degree, int(d.max()))\n",
    "            # Compute the in-degree histogram tensor\n",
    "            deg = torch.zeros(max_degree + 1, dtype=torch.long)\n",
    "            for data in self.transformed_mols:\n",
    "                d = degree(\n",
    "                    data.edge_index[1], num_nodes=data.num_nodes, dtype=torch.long\n",
    "                )\n",
    "                deg += torch.bincount(d, minlength=deg.numel())\n",
    "            self._degrees = deg\n",
    "        return self._degrees\n",
    "\n",
    "    def collate_fn(self, **kwargs):\n",
    "        # luckily the molfeat featurizer provides a collate function for PyG\n",
    "        return self.featurizer.get_collate_fn(**kwargs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.transformed_mols[index], self.y[index]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now process the dataset with our custom class, and split into test and train datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DTset(df.smiles.values, df.exp.values, featurizer)\n",
    "# The line below can be used for the QM9 dataset - or edit this line for your own dataset as suggested above.\n",
    "# dataset = DTset(df.smiles.values, df.gap.values, featurizer)\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "train_dt, test_dt = torch.utils.data.random_split(\n",
    "    dataset, [0.8, 0.2], generator=generator\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the PopTorch dataloader we will need a fixed batch size, so for starters we can make a conservative estimate of the maximum possible batch size as the maximum number of nodes or edges multiplied by the batch size. (For a more efficient batch see the tutorial [here](https://console.paperspace.com/github/graphcore/Gradient-Pytorch-Geometric?machine=Free-IPU-POD4&container=graphcore%2Fpytorch-geometric-jupyter%3A3.2.0-ubuntu-20.04-20230314&file=%2Flearning-pytorch-geometric-on-ipus%2F4_small_graph_batching_with_packing.ipynb&utm_source=Medium&utm_medium=content&utm_campaign=PyG+Launch) for packed batches.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_nodes_edges(dataset):\n",
    "    max_nodes, max_edges = 0, 0\n",
    "\n",
    "    for data in dataset:\n",
    "        data = data[0]\n",
    "        num_nodes = data.num_nodes\n",
    "        num_edges = data.num_edges\n",
    "\n",
    "        max_nodes = max(max_nodes, num_nodes)\n",
    "        max_edges = max(max_edges, num_edges)\n",
    "\n",
    "    return max_nodes, max_edges\n",
    "\n",
    "\n",
    "max_nodes, max_edges = max_nodes_edges(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The custom dataset object has a collate function that utilizes the collate function provided by the featurizer, we want to combine this with the poptorch `CustomFixedSizeDataLoader` that ensures the batches are all of fixed size. To do this we can write a light custom DataLoader - it calls the collate function from the `CustomFixedSizeDataLoader` then calls the collate function from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from poptorch_geometric.dataloader import CustomFixedSizeDataLoader\n",
    "\n",
    "\n",
    "class CombinedDataloader(CustomFixedSizeDataLoader):\n",
    "    def _create_collater(self, **collater_args):\n",
    "        fixed_size_collater = super()._create_collater(**collater_args)\n",
    "        featurizer_collater_fn = dataset.collate_fn(return_pair=False)\n",
    "\n",
    "        def my_collater(input):\n",
    "            input = featurizer_collater_fn(input)\n",
    "            input = fixed_size_collater(input)\n",
    "            return input\n",
    "\n",
    "        return my_collater"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collate_fn = dataset.collate_fn(return_pair=False)\n",
    "from torch_geometric.transforms import Pad\n",
    "\n",
    "pad_transform = Pad(\n",
    "    max_num_nodes=max_nodes * BATCH_SIZE, max_num_edges=max_edges * BATCH_SIZE\n",
    ")\n",
    "\n",
    "\n",
    "def padded_collate_fn(data_list):\n",
    "    print(data_list)\n",
    "    batch = collate_fn(data_list)\n",
    "    batch = pad_transform(batch)\n",
    "    print(batch)\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from poptorch_geometric.dataloader import DataLoader\n",
    "\n",
    "train_opts = poptorch.Options()\n",
    "train_opts.deviceIterations(1)\n",
    "train_opts.Training.gradientAccumulation(1)\n",
    "train_opts.replicationFactor(1)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dt,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    drop_last=True,\n",
    "    options=train_opts,\n",
    "    collate_fn=padded_collate_fn,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can use the new dataloader to create the `train_loader` and `test_loader` iterables that we will use in training. The args we pass in for the collate function are as normal for poptorch Geometric, but now we can have a fixed size batch with all the utility from the featurizer. \n",
    "\n",
    "Here we set the poptorch options for both training and testing, for simplciity we are going to set device iteration, gradient accumulation, and number of replicas all to 1. For more information on tuning these parameters, you can look [here](https://console.paperspace.com/github/graphcore/Gradient-Pytorch-Geometric?machine=Free-IPU-POD4&container=graphcore%2Fpytorch-geometric-jupyter%3A3.2.0-ubuntu-20.04-20230314&file=%2Flearning-pytorch-geometric-on-ipus%2F1_at_a_glance.ipynb&utm_source=Medium&utm_medium=content&utm_campaign=PyG+Launch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_opts = poptorch.Options()\n",
    "train_opts.deviceIterations(1)\n",
    "train_opts.Training.gradientAccumulation(1)\n",
    "train_opts.replicationFactor(1)\n",
    "\n",
    "test_opts = poptorch.Options()\n",
    "test_opts.deviceIterations(1)\n",
    "test_opts.Training.gradientAccumulation(1)\n",
    "test_opts.replicationFactor(1)\n",
    "\n",
    "train_loader = CombinedDataloader(\n",
    "    dataset=train_dt,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_nodes=max_nodes * BATCH_SIZE,\n",
    "    collater_args=dict(num_edges=max_edges * BATCH_SIZE, add_masks_to_batch=True),\n",
    "    drop_last=True,\n",
    "    options=train_opts,\n",
    ")\n",
    "\n",
    "test_loader = CombinedDataloader(\n",
    "    dataset=test_dt,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_nodes=max_nodes * BATCH_SIZE,\n",
    "    collater_args=dict(num_edges=max_edges * BATCH_SIZE, add_masks_to_batch=True),\n",
    "    drop_last=True,\n",
    "    options=test_opts,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see how the collater works, we can inspect a single batch. We can see that the batch has been padded to the fixed size we specified and a mask has been added for the additional nodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(iter(train_loader))\n",
    "sample"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network + Training\n",
    "We are almost ready to go, we just need to define our GNN. For this example we are going to build a [GINE](https://arxiv.org/abs/1905.12265) model. \n",
    "We use GINE because it allows us to exploit the edge features provided by Molfeat.\n",
    "The PyTorch Geometric docs give a good description of the core `GINEConv` layer [here](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GINEConv.html#torch_geometric.nn.conv.GINEConv).\n",
    "\n",
    "This model is a further extension of the `GIN` model used in the `<note/book>.ipynb`.\n",
    "If you are unfamiliar with the adaptions needed when building a PyTorch model on the IPU, then please see our tutorials. The main difference with vanilla PyTorch is that the loss is returned by the model itself as can be seen on `line 71` in the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch_geometric.nn.conv import GINConv, GINEConv\n",
    "from torch_geometric.nn.pool import global_add_pool\n",
    "from torch_geometric.nn.models import MLP\n",
    "\n",
    "\n",
    "class GINE(nn.Module):\n",
    "    \"\"\"\n",
    "    Graph Isomorphism Network modified to take into account fixed size\n",
    "    tensor inputs created with padding.\n",
    "\n",
    "    params:\n",
    "        in_channels (int): number of features each node is represented by\n",
    "        hidden_channels (int): number of hidden units for all MLP layers\n",
    "        out_channels (int): num of hidden units in the output of the network\n",
    "        num_conv_layers (int): number of GINConv layers in the network\n",
    "        num_mlp_layers (int): number of hidden layers in MLP\n",
    "        batch_size (int): maximum number of graphs in a batch\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        hidden_channels,\n",
    "        out_channels,\n",
    "        num_conv_layers,\n",
    "        num_mlp_layers,\n",
    "        batch_size,\n",
    "        edge_dim,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # `num_conv_layers` layers for AGGREGATE and COMBINE\n",
    "        self.hop_k_gin_layers = nn.ModuleList()\n",
    "\n",
    "        # linear READOUT nets for (sum) graph pooling\n",
    "        # the first pooling occurs on the input nodes' features (0-hop)\n",
    "        self.hop_k_readout_layers = nn.ModuleList(\n",
    "            [nn.Linear(in_features=in_channels, out_features=out_channels)]\n",
    "        )\n",
    "\n",
    "        # Final Regression Head\n",
    "        self.out_layer = nn.Linear(in_features=out_channels, out_features=1)\n",
    "\n",
    "        for k_hop in range(num_conv_layers):\n",
    "            phi = MLP(\n",
    "                in_channels=in_channels if k_hop == 0 else hidden_channels,\n",
    "                hidden_channels=hidden_channels,\n",
    "                out_channels=hidden_channels,\n",
    "                num_layers=num_mlp_layers,\n",
    "                act=\"relu\",\n",
    "                norm=\"layer_norm\",\n",
    "                plain_last=False,\n",
    "            )\n",
    "\n",
    "            # performs the initial (sum) neighbour pooling + (1-eps) * hv, then applies phi\n",
    "            # i.e phi o f = MLP((1-eps)*hv + (sum) neighbour k_hop representation)\n",
    "            self.hop_k_gin_layers.append(\n",
    "                GINEConv(nn=phi, eps=0, train_eps=False, edge_dim=edge_dim)\n",
    "            )\n",
    "\n",
    "            # READOUT is performed on each k_hop node representation for each graph\n",
    "            self.hop_k_readout_layers.append(\n",
    "                nn.Linear(in_features=hidden_channels, out_features=out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(\n",
    "        self, x, edge_index, batch, graphs_mask=None, target=None, edge_attr=None\n",
    "    ):\n",
    "        # perform k-hop aggregation using GINConv, and return all layer outputs\n",
    "        hop_k_outputs = [x]\n",
    "        h = x\n",
    "        for gin_layer in self.hop_k_gin_layers:\n",
    "            h = gin_layer(h, edge_index, edge_attr)\n",
    "            hop_k_outputs.append(h)\n",
    "\n",
    "        # perform readout over all nodes in each graph in each layer\n",
    "        score_over_layer = torch.zeros((1))\n",
    "        for i, linear in enumerate(self.hop_k_readout_layers):\n",
    "            pooled_h = global_add_pool(\n",
    "                x=hop_k_outputs[i], batch=batch, size=self.batch_size\n",
    "            )\n",
    "            # compute scores\n",
    "            score_over_layer = score_over_layer + nn.functional.dropout(\n",
    "                linear(pooled_h), training=self.training\n",
    "            )\n",
    "        score_over_layer = self.out_layer(score_over_layer)\n",
    "        if self.training:\n",
    "            # Compute loss\n",
    "            loss = F.mse_loss(score_over_layer.squeeze()[:-1], target[:-1])\n",
    "            return score_over_layer, loss\n",
    "\n",
    "        return score_over_layer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the training model \n",
    "\n",
    "Now we create the model for the prediction task and prepare it for training.\n",
    "\n",
    "We can tune a number of key parameters below:\n",
    "* `LEARNING_RATE` - set the learning rate for the model\n",
    "* `EPOCHS` - training duration\n",
    "* `HIDDEN_CHANNELS`  - hidden dimension size of the MLP in the GINE model \n",
    "* `OUT_CHANNELS` - out dimension from the message passing portion of the layer\n",
    "* `NUM_CONV_LAYERS` - number of conv layers (i.e. message passing steps)\n",
    "* `NUM_MLP_LAYERS` - number of final MLP layers after each conv block\n",
    "\n",
    "(Note: Each conv layer adds an additional linear readout layer as well)\n",
    "\n",
    "We can see the impact of model size by tweaking these parameters in the summary printed below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tunable Parameters\n",
    "LEARNING_RATE = 5e-4\n",
    "NUM_EPOCHS = 15\n",
    "HIDDEN_CHANNELS = 256\n",
    "OUT_CHANNELS = 256  # dataset.num_output\n",
    "NUM_CONV_LAYERS = 4\n",
    "NUM_MLP_LAYERS = 2\n",
    "\n",
    "\n",
    "model = GINE(\n",
    "    dataset.num_atom_features,\n",
    "    HIDDEN_CHANNELS,\n",
    "    OUT_CHANNELS,\n",
    "    NUM_CONV_LAYERS,\n",
    "    NUM_MLP_LAYERS,\n",
    "    BATCH_SIZE,\n",
    "    dataset.num_bond_features,\n",
    ")\n",
    "model.train()\n",
    "\n",
    "optimizer = poptorch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "poptorch_training_model = poptorch.trainingModel(\n",
    "    model, options=train_opts, optimizer=optimizer\n",
    ")\n",
    "\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "summary(poptorch_training_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model \n",
    "\n",
    "Now we can start training the model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "epoch_losses = []\n",
    "with tqdm(range(NUM_EPOCHS), colour=\"#FF6F79\") as pbar:\n",
    "    for epoch in pbar:\n",
    "        losses = []\n",
    "        for data in train_loader:\n",
    "            out, loss = poptorch_training_model(\n",
    "                data.x,\n",
    "                data.edge_index,\n",
    "                batch=data.batch,\n",
    "                graphs_mask=data.graphs_mask,\n",
    "                edge_attr=data.edge_attr,\n",
    "                target=data.y,\n",
    "            )\n",
    "            losses.append(loss.item())\n",
    "            epoch_losses.append(loss.item())\n",
    "        print(f\"Epoch {epoch} - Loss {np.mean(losses):.3f}\")\n",
    "\n",
    "        pbar.set_description(f\"Epoch {epoch} - Loss {np.mean(losses):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poptorch_training_model.detachFromDevice()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's always good to look at the curve of the training loss\n",
    "plot_smoothed_loss(epoch_losses)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "We can now test our model. For the simplicity of this tutorial, no hyperparameter search or evaluation of the best atom/bond featurization was performed. This inevitably impacts the performance - but is left as an exercise to the reader.\n",
    "\n",
    "A manual hyperparameter search is easy to start performing by tweaking the given hyperparameters when building the model above and running the training loop again. (Remember to run both the building cell and the training cell.)\n",
    "The model compiles and trains in a couple of minutes on the IPU, so it's easy to experiment with even quite large models quickly and tune the model to your specific dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "model.eval()\n",
    "poptorch_inference_model = poptorch.inferenceModel(model, options=test_opts)\n",
    "test_y_hat = []\n",
    "test_y_true = []\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        out = poptorch_inference_model(\n",
    "            data.x,\n",
    "            data.edge_index,\n",
    "            batch=data.batch,\n",
    "            graphs_mask=data.graphs_mask,\n",
    "            edge_attr=data.edge_attr,\n",
    "        )[:-1]\n",
    "        # out = global_add_pool(out, data.batch)\n",
    "        test_y_hat.append(out.detach().cpu().squeeze())\n",
    "        test_y_true.append(data.y[:-1])\n",
    "\n",
    "\n",
    "test_y_hat = torch.cat(test_y_hat).numpy()\n",
    "test_y_true = torch.cat(test_y_true).numpy()\n",
    "\n",
    "r2 = r2_score(test_y_true, test_y_hat)\n",
    "mae = mean_absolute_error(test_y_true, test_y_hat)\n",
    "poptorch_inference_model.detachFromDevice()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualise the distribution of results by plotting the predicted values against the expected true values from the dataset. The $R^2$ and MAE are given on the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_contours\n",
    "\n",
    "plot_contours(test_y_true, test_y_hat, r2, mae)\n",
    "\n",
    "print(min(test_y_hat), max(test_y_hat))\n",
    "print(min(test_y_true), max(test_y_true))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo Molecules\n",
    "\n",
    "We can see the MAE and $R^2$ score above, and while they seem pretty good, it's hard to put that in context of a real use case. \n",
    "So instead let's apply the model to individual molecules and report the performance.\n",
    "\n",
    "For demonstration purposes we'll build a new dataloader for inference with a batch-size of 1 molecule, and for simplicity we'll just take the full dataset - in reality this might be a different dataset, or new molecules as they are needed to be evaluated , but this gives an idea of the workflow. \n",
    "\n",
    "This is a similar idea to the demo in the [Transformer notebook](https://console.paperspace.com/github/graphcore/Gradient-HuggingFace?machine=Free-IPU-POD4&container=graphcore/pytorch-jupyter%3A3.2.0-ubuntu-20.04-20230331&file=dolly2-instruction-following%2FDolly2-an-OSS-instruction-LLM.ipynb), but here we need to provide a little more informarion to ensure the graphs are fixed size and will work with any molecule in our dataset. The batch size is 2 to allow for the dummy padding graph (see tutorial X for more information) and we provide a really generous max nodes + edges which for most graphs is overkill, but keeps the dataloader simple, and in a single molecule by single molecule case is perfectly sufficient for our needs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_opts = poptorch.Options()\n",
    "inf_opts.deviceIterations(1)\n",
    "inf_opts.replicationFactor(1)\n",
    "inf_df = df\n",
    "# These two lines are can be swapped to choose the QM9 dataset as an extension\n",
    "dataset = DTset(inf_df.smiles.values, inf_df.exp.values, featurizer)\n",
    "# dataset = DTset(inf_df.smiles.values, inf_df.gap.values, featurizer)\n",
    "\n",
    "inf_loader = CombinedDataloader(\n",
    "    dataset=dataset,\n",
    "    batch_size=2,\n",
    "    num_nodes=max_nodes * BATCH_SIZE,\n",
    "    collater_args=dict(num_edges=max_edges * BATCH_SIZE, add_masks_to_batch=True),\n",
    "    drop_last=True,\n",
    "    options=inf_opts,\n",
    ")\n",
    "model.batch_size = 2\n",
    "model.eval()\n",
    "inference_model = poptorch.inferenceModel(model, options=inf_opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = iter(inf_loader)\n",
    "smiles = iter(df.smiles.values)\n",
    "# These lines can be selected to run QM9 dataset if desired\n",
    "# names = iter(df.mol_id.values)\n",
    "names = iter(df.CMPD_CHEMBLID.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import Emoji\n",
    "\n",
    "\n",
    "def next_molecule():\n",
    "    # =============================================================\n",
    "    # |                                                           |\n",
    "    # |             DEMO MOLECULE PREDICTION                      |\n",
    "    # |               (Re-run this block)                         |\n",
    "    # |                                                           |\n",
    "    # =============================================================\n",
    "    clear_output(True)\n",
    "    sample = next(sampler)\n",
    "    name = next(names)\n",
    "    smile = next(smiles)\n",
    "    # print(name)\n",
    "    out = inference_model(\n",
    "        sample.x,\n",
    "        sample.edge_index,\n",
    "        batch=sample.batch,\n",
    "        graphs_mask=data.graphs_mask,\n",
    "        edge_attr=sample.edge_attr,\n",
    "    )\n",
    "    view = report_molecule_regression(name, sample.y[0], out.squeeze()[0], smile)\n",
    "    view.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RE-RUN THIS CELL TO SEE INFERENCE RESULTS ON INDIVIDUAL MOLECULES\n",
    "\n",
    "next_molecule()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook we've shown how to use pretrained feature to train a model on a downstream task, including how to use this model with new molecules.\n",
    "\n",
    "**Next steps:**\n",
    "- Try a hyperparameter sweep - in the section on building the model there are some suggested parameters to try tuning. Try changing the learning rate or number of hidden layers and see the impact of the training dynamics.  \n",
    "- Try changing the dataset - this example start with the `Lipophilicity` dataset (LogD) from MoleculeNet, and the code is provided in comments with hints explaining how to update to train on the `QM9` dataset for a regression task. \n",
    "- Try exploring more datasets from [MoleculeNet](https://moleculenet.org/datasets-1) which are labeled by the type of task and see if you can tune the fine-tuning on these datasets. \n",
    "- Try a different model from MolFeat - the `ChemGPT-4.7M` model is provided as an alternative featurizer / model to finetune - this is in the same cell as the `QM9` dataset.\n",
    "\n",
    "- For an alternative use of the MolFeat library please see the integration of MolFeat with PyTorch Geometric on the IPU in  `<folder_name/notebook_name>.ipynb`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally detach the inference model - this line is at the end to make sure it's not run accidentally before finishing the notebook.\n",
    "inference_model.detachFromDevice()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('3.2.0+1277_poptorch': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dbfc712368f9f170a0635e201b20e8278cfe4185a9064a095e68346a2c2a6bf6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
