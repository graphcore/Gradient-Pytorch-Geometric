{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "236726a1",
   "metadata": {},
   "source": [
    "Copyright (c) 2023 Graphcore Ltd. All rights reserved."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3e1fbc52",
   "metadata": {},
   "source": [
    "Prediction of Molecular Properties using SchNet on Graphcore IPUs\n",
    "=================================================================\n",
    "\n",
    "This notebook demonstrates training a [SchNet graph neural network](https://arxiv.org/abs/1712.06113) with PyTorch Geometric on the Graphcore IPU.  We will use the QM9 dataset from the [MoleculeNet: A Benchmark for Molecular\n",
    "    Machine Learning](https://arxiv.org/abs/1703.00564) paper and train the SchNet model to predict the HOMO-LUMO energy gap.\n",
    "\n",
    "This notebook assumes some familiarity with PopTorch as well as PyTorch Geometric (PyG).  For additional resources please consult:\n",
    "\n",
    "* [PopTorch Documentation](https://docs.graphcore.ai/projects/poptorch-user-guide/en/latest/index.html)\n",
    "* [PopTorch Examples and Tutorials](https://docs.graphcore.ai/en/latest/examples.html#pytorch)\n",
    "* [PyTorch Geometric](https://pytorch-geometric.readthedocs.io/en/stable/)\n",
    "* [PopTorch Geometric Documentation](https://docs.graphcore.ai/projects/poptorch-geometric-user-guide/en/latest/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bef521",
   "metadata": {},
   "source": [
    "### Running on Paperspace\n",
    "\n",
    "The Paperspace environment lets you run this notebook with no set up. To improve your experience we preload datasets and pre-install packages, this can take a few minutes, if you experience errors immediately after starting a session please try restarting the kernel before contacting support. If a problem persists or you want to give us feedback on the content of this notebook, please reach out to through our community of developers using our [slack channel](https://www.graphcore.ai/join-community) or raise a [GitHub issue](https://github.com/graphcore/examples).\n",
    "\n",
    "Requirements:\n",
    "\n",
    "* Python packages installed with `pip install -r requirements.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0037bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fa1d4d",
   "metadata": {},
   "source": [
    "Lets import the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fd5fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "import poptorch\n",
    "import pandas as pd\n",
    "import py3Dmol\n",
    "\n",
    "from periodictable import elements\n",
    "from poptorch_geometric.dataloader import CustomFixedSizeDataLoader\n",
    "from torch_geometric.datasets import QM9\n",
    "from torch_geometric.data import Batch\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import to_fixed_size\n",
    "from torch_geometric.nn.models import SchNet\n",
    "from torch_geometric.transforms import Pad\n",
    "from torch_geometric.transforms.pad import AttrNamePadding\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils import TrainingModule, KNNInteractionGraph, prepare_data, optimize_popart\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme()\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5074635",
   "metadata": {},
   "source": [
    "And for compatibility with the Paperspace environment variables we will do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec24dcb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "poptorch.setLogLevel(\"ERR\")\n",
    "executable_cache_dir = (\n",
    "    os.getenv(\"POPLAR_EXECUTABLE_CACHE_DIR\", \"/tmp/exe_cache/\") + \"/pyg-schnet\"\n",
    ")\n",
    "dataset_directory = os.getenv(\"DATASET_DIR\", \"data\")\n",
    "num_ipus = os.getenv(\"NUM_AVAILABLE_IPU\", \"4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc702aa",
   "metadata": {},
   "source": [
    "Now we are ready to start!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f5f8f49e",
   "metadata": {},
   "source": [
    "### QM9 Dataset\n",
    "\n",
    "PyG provides a convenient dataset class that manages downloading the [QM9 dataset](https://pytorch-geometric.readthedocs.io/en/stable/modules/datasets.html#torch_geometric.datasets.QM9) to local storage. The QM9 dataset contains 130831 molecules with a number of different physical properties that we can train the SchNet model to predict.  For SchNet, a molecule with $n$ atoms is described by:\n",
    "\n",
    "* Nuclear charges $Z= (Z_1, Z_2, ..., Z_n)$, stored as a vector of integers of length `num_atoms`\n",
    "* Atomic positions $\\mathbf{r} = (\\mathbf{r}_1, \\mathbf{r}_2, \\ldots, \\mathbf{r}_n)$, stored as a tensor of real numbers of size `[num_atoms, 3]`\n",
    "\n",
    "We consider each molecule as an undirected graph where:\n",
    "* the atoms are the nodes or vertices of the graph.\n",
    "* the edges are inferred from the atomic positions by connecting atoms that are within a given cutoff radius to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f88bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "qm9_root = osp.join(dataset_directory, \"qm9\")\n",
    "dataset = QM9(qm9_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2496b4",
   "metadata": {},
   "source": [
    "We can call `len` to see how many molecules are in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348094dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0c89130e",
   "metadata": {},
   "source": [
    "We can inspect each molecule which is represented as an instance of a [torch_geometric.data.Data](https://pytorch-geometric.readthedocs.io/en/stable/modules/data.html#torch_geometric.data.Data) object.  The properties we are interested in for training our SchNet model are:\n",
    "* `z` contains the atomic number for each atom in the molecule.\n",
    "* `pos` contains the 3d structure of the molecule.\n",
    "* `y` contains the 19 regression targets.  The HOMO-LUMO gap is stored in the 4th column so can be accessed by slicing this tensor using `y[:,4]`\n",
    "\n",
    "Next we display the first example molecule from the QM9 dataset as a `Data` object, the atomic number tensor, the positions tensor, and slice the regression targets to get the HOMO-LUMO gap for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b36c4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "datum = dataset[123244]\n",
    "datum, datum.z, datum.pos, datum.y[:, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8c7d3c",
   "metadata": {},
   "source": [
    "Using a transform to the QM9 dataset we can select the properties we need for training SchNet and convert them to a PyG Data object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f22f690",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.transform = prepare_data\n",
    "dataset[123244]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2490229",
   "metadata": {},
   "source": [
    "We can use the [py3Dmol](https://github.com/3dmol/3Dmol.js/tree/master/py3Dmol) package to visualise the molecule to get a better idea of the structure.  To do this we need to provide the simple `xyz` format to the `py3Dmol.view` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfd7feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_atoms = int(datum.z.numel())\n",
    "xyz = f\"{num_atoms}\\n\\n\"\n",
    "\n",
    "for i in range(num_atoms):\n",
    "    sym = elements[datum.z[i].item()].symbol\n",
    "    r = datum.pos[i, :].tolist()\n",
    "    line = [sym] + [f\"{i: 0.08f}\" for i in r]\n",
    "    line = \"\\t\".join(line)\n",
    "    xyz += f\"{line}\\n\"\n",
    "\n",
    "view = py3Dmol.view(data=xyz, style={\"stick\": {}})\n",
    "view.spin()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcbc7fa",
   "metadata": {},
   "source": [
    "Next we collect some statistics by iterating over the entire dataset and investigate the distribution of the number of atoms in each molecule and the HOMO-LUMO gap energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7775796",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_mols = len(dataset)\n",
    "num_atoms = []\n",
    "hl_gap = []\n",
    "\n",
    "for mol in tqdm(dataset):\n",
    "    num_atoms.append(mol.z.numel())\n",
    "    hl_gap.append(float(mol.y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af436c4d",
   "metadata": {},
   "source": [
    "Create a pandas dataframe from the collected statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15673ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"Number of atoms\": num_atoms, \"HOMO-LUMO Gap (eV)\": hl_gap})\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851aa9c0",
   "metadata": {},
   "source": [
    "The following figure shows how the number of atoms varies across the QM9 dataset as well as the kernel density estimate (KDE) of the HOMO-LUMO gap energy.The following histogram shows how the number of atoms varies across the QM9 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0197f982",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = plt.figure(figsize=[10, 4])\n",
    "sns.histplot(data=df, x=df.columns[0], ax=plt.subplot(1, 2, 1), discrete=True)\n",
    "sns.kdeplot(data=df, x=df.columns[1], ax=plt.subplot(1, 2, 2))\n",
    "h.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3e404c0b",
   "metadata": {},
   "source": [
    "## Data Loading and Batching\n",
    "\n",
    "PyG provides a specialized version of the native PyTorch [torch.utils.data.DataLoader](https://pytorch.org/docs/stable/data.html):\n",
    "\n",
    "* [torch_geometric.data.DataLoader](https://pytorch-geometric.readthedocs.io/en/latest/modules/data.html#torch_geometric.data.DataLoader)\n",
    "\n",
    "\n",
    "The PyG dataloader supports a form of mini-batching which is [described here](https://pytorch-geometric.readthedocs.io/en/stable/notes/batching.html).  Effectively each mini-batch is a concatenation of multiple graphs (molecules in QM9).  Another way to understand this is that each mini-batch is one large graph comprised of multiple disconnected sub-graphs.  The PyG dataloader will generate a `batch` vector that assigns each feature in the mini-batch into a distinct subgraph.  This is useful for message passing networks (such as SchNet) and pooling layers to produce a distinct regression prediction for each molecule. Refer to the following tutorials for additional background:\n",
    "\n",
    "* [Creating message passing networks](https://pytorch-geometric.readthedocs.io/en/stable/notes/create_gnn.html)\n",
    "* [Global Pooling Layers](https://pytorch-geometric.readthedocs.io/en/stable/modules/nn.html?highlight=pooling#global-pooling-layers)\n",
    "\n",
    "This mini-batching approach needs to be adapted for the IPU since the tensor sizes will vary from batch to batch.  This can be observed in the following cell where tensors such as `pos`, `z`, and `batch` all have different sizes between the first two batches of the QM9 dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0dbac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(dataset, batch_size=4)\n",
    "\n",
    "it = iter(loader)\n",
    "next(it), next(it)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ac74c732",
   "metadata": {},
   "source": [
    "### SchNet Model Architecture\n",
    "\n",
    "![SchNet Architecture](./static/schnet_arch.png \"SchNet Architecture\")\n",
    "\n",
    "The diagram above demonstrates the overall architecture of the SchNet model.  The main inputs to the model are:\n",
    "* $(Z_1, Z_2, \\ldots, Z_n)$ : A vector of atomic numbers which are used as input to the atom-wise embedding layer.\n",
    "* $(\\mathbf{r}_1, \\mathbf{r}_2, \\ldots, \\mathbf{r}_n)$: An `[n, 3]` tensor of atomic positions.\n",
    "\n",
    "The graph is defined by considering each atom as a node and the edges are defined by:\n",
    "* placing a sphere of radius $r_\\textrm{cut}$ centered on each atom.\n",
    "* connecting neighbouring atoms that fall within the cutoff sphere with an undirected edge.\n",
    "\n",
    "The rationale for using this cutoff sphere is to bound the maximum number of atoms that are connected to each other so that the computational cost grows linearly with the number of atoms in the system.\n",
    "\n",
    "By default the inter-atomic interaction graph will be computed using the `radius_graph` [method](https://pytorch-geometric.readthedocs.io/en/stable/modules/nn.html#torch_geometric.nn.pool.radius_graph) in PyTorch Geometric."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c1073540",
   "metadata": {},
   "source": [
    "### IPU implementation of SchNet\n",
    "\n",
    "General support for PyTorch on the IPU is accomplished through ahead-of-time compilation with PopTorch. The compiler performs static analysis over the input tensors and the computational graph to optimise the evaluation on the IPU.  To fully leverage these optimisations for the SchNet architecture we need to enforce consistent tensors sizes for all:\n",
    "* operations used within the model.\n",
    "* mini-batches of molecular graphs that are inputs to the model.\n",
    "\n",
    "We first identify that the default interaction graph method using `radius_graph` will by definition create a variable number of edges depending on the geometric structure of the molecule.  This is unfriendly for the ahead-of-time compilation in PopTorch but we can reformulate the interaction graph as a k-nearest neighbours search.  We use the `interaction_graph` argument to the PyTorch Geometric SchNet [implementation](https://pytorch-geometric.readthedocs.io/en/stable/modules/nn.html#torch_geometric.nn.models.SchNet) to compute the pairwise interaction graph and interatomic distances.\n",
    "\n",
    "We can use a simple strategy of appending a padding graph to effectively fill up each mini-batch up to a known maximum possible size.  To accomplish this we need to define non-interacting padding atoms.  These padding atoms are defined as having atomic charge zero.  This ensures there are no artificial interactions introduced between these padding atoms and any real atoms within the mini-batch.\n",
    "\n",
    "Refer `pyg_schnet_util.py` file for the implementation details that are needed to fully realise an efficient evaluation of the SchNet GNN on the IPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84f0dad",
   "metadata": {},
   "source": [
    "As a basic sanity check we can compile the SchNet model with PopTorch and check that we get the same prediction as running the model on the host CPU.\n",
    "\n",
    "Prepare a mock batch consisting of a single graph using the PyG `Batch.from_data_list` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96bb999",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = Batch.from_data_list([dataset[0]])\n",
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719924fa",
   "metadata": {},
   "source": [
    "Evaluate the network on the CPU with randomly initialised weights using a fixed random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f00fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "cutoff = 10.0\n",
    "model = SchNet(cutoff=cutoff)\n",
    "model.eval()\n",
    "cpu = model(batch.z, batch.pos, batch.batch)\n",
    "cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f09cbab",
   "metadata": {},
   "source": [
    "Now evaluate the network on the IPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46500d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "knn_graph = KNNInteractionGraph(cutoff=cutoff, k=batch.num_nodes - 1)\n",
    "model = SchNet(cutoff=cutoff, interaction_graph=knn_graph)\n",
    "model = to_fixed_size(model, batch_size=1)\n",
    "options = poptorch.Options()\n",
    "options.enableExecutableCaching(executable_cache_dir)\n",
    "pop_model = poptorch.inferenceModel(model, options)\n",
    "ipu = pop_model(batch.z, batch.pos, batch.batch)\n",
    "\n",
    "ipu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebb1c54",
   "metadata": {},
   "source": [
    "The predictions must be the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3349f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.allclose(cpu, ipu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1decc9",
   "metadata": {},
   "source": [
    "### Padding\n",
    "The easiest way to get up and running on the IPU with the QM9 dataset is just to apply padding to the input tensors. This results in every example in the dataset being expanded up to the size of the largest molecule.  This expansion comes at the cost of additional padding nodes and edges.\n",
    "\n",
    "We use the `Pad` [transform](https://pytorch-geometric.readthedocs.io/en/latest/modules/transforms.html).  This transform modifies each `Data` instance in the dataset to have `max_num_atoms` nodes. Specifically we pad the `z` and `pos` items with zeroes. We also pad the `batch` item with 1, to ensure that the extra padded nodes added to our data refer to a separate graph to the original graph and so do not interfere during operations such as pooling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0b7838",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Batch.from_data_list([dataset[0]])\n",
    "pad_transform = Pad(32, node_pad_value=AttrNamePadding({\"z\": 0, \"pos\": 0, \"batch\": 1}))\n",
    "padded_batch = pad_transform(data)\n",
    "padded_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7f6328",
   "metadata": {},
   "source": [
    "The next sanity check is to verify that the padding hasn't introduced any numerical artifacts in the resulting prediction.  For this we will use our mock padded_batch above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0596ae96",
   "metadata": {},
   "source": [
    "Evaluate the network on the host with randomly initialised weights using a fixed random seed and the padded batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5995ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "model = SchNet(cutoff=cutoff)\n",
    "model.eval()\n",
    "padded_cpu = model(padded_batch.z, padded_batch.pos, padded_batch.batch)\n",
    "padded_cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ea0187",
   "metadata": {},
   "source": [
    "The result should be the same as the one we calculated earlier without any padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26ae4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.allclose(cpu, padded_cpu[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272746b4",
   "metadata": {},
   "source": [
    "Now evaluate the same test using the IPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f709345d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "knn_graph = KNNInteractionGraph(cutoff=cutoff, k=batch.num_nodes - 1)\n",
    "model = SchNet(cutoff=cutoff, interaction_graph=knn_graph)\n",
    "model = to_fixed_size(model, batch_size=2)\n",
    "pop_model = poptorch.inferenceModel(model, options)\n",
    "padded_ipu = pop_model(batch.z, batch.pos, batch.batch)\n",
    "\n",
    "padded_ipu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000c3728",
   "metadata": {},
   "source": [
    "The predictions must be the same as calculated earlier without any paddding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722d2121",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.allclose(ipu, padded_ipu[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2642af5",
   "metadata": {},
   "source": [
    "Detach the inference model from the IPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f712a8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_model.detachFromDevice()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4fb02ad3",
   "metadata": {},
   "source": [
    "### Efficient data loading for the IPU\n",
    "\n",
    "PopTorch provides a custom data loader implementation that can be used for efficient data batching and transfers between the host and IPU device.  Please refer to the following resources for additional background:\n",
    "* PopTorch documentation [Efficient data batching](https://docs.graphcore.ai/projects/poptorch-user-guide/en/latest/batching.html#efficient-data-batching)\n",
    "* PopTorch tutorial: [Efficient data loading](https://github.com/graphcore/tutorials/tree/sdk-release-2.5/tutorials/pytorch/tut2_efficient_data_loading)\n",
    "\n",
    "We can use PopTorch Geometric to make use of these for our models built in PyTorch Geometric.\n",
    "\n",
    "Instead of using the `Pad` transform we used above to make a single item fixed size, we can use a `CustomFixedSizeDataLoader` from PopTorch Geometric. This will pad our batch of graphs rather than padding each item. One key thing to note is we set the number of nodes we pad to to our maximum number of nodes in the dataset multiplied by the batch size minus 1. This is because the final graph in our batch will be allocated for padding and will not be a real graph from the dataset. For more information on using these tools to achieve fixed size data, see our [Small Graph batching using Padding tutorial](../../../tutorials/tutorials/pytorch_geometric/3_small_graph_batching_with_padding/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b350656c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "\n",
    "dataloader = CustomFixedSizeDataLoader(\n",
    "    dataset, batch_size=batch_size, num_nodes=32 * (batch_size - 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1741f23",
   "metadata": {},
   "source": [
    "Now taking a look at the first couple of items produced by the dataloader you can see they are the same size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57238ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_iter = iter(dataloader)\n",
    "first_batch = next(dataloader_iter)\n",
    "second_batch = next(dataloader_iter)\n",
    "print(first_batch)\n",
    "print(second_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6556e60",
   "metadata": {},
   "source": [
    "By inspecting the `batch` data item you can see that many nodes corresponding to graph id `7`. This graph is a padded graph making up the batch to a fixed size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322ab272",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_batch.batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802343d4",
   "metadata": {},
   "source": [
    "Now we have all the pieces we need to train SchNet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004cdcde",
   "metadata": {},
   "source": [
    "### Putting everything together to train SchNet\n",
    "\n",
    "We can now train SchNet on the IPU using all of the concepts introduced earlier.  To start with we shuffle and split the dataset into testing, validation, and training splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc11e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_test = 10000\n",
    "num_val = 10000\n",
    "torch.manual_seed(0)\n",
    "dataset = dataset.shuffle()\n",
    "test_dataset = dataset[:num_test]\n",
    "val_dataset = dataset[num_test : num_test + num_val]\n",
    "train_dataset = dataset[num_test + num_val :]\n",
    "\n",
    "print(\n",
    "    f\"Number of test molecules: {len(test_dataset)}\\n\"\n",
    "    f\"Number of validation molecules: {len(val_dataset)}\\n\"\n",
    "    f\"Number of training molecules: {len(train_dataset)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fda988",
   "metadata": {},
   "source": [
    "Setup the hyperparameters for training the network.  These can be changed to explore the different trade-offs they offer in terms of training accuracy and performance throughput."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabf3d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "replication_factor = int(num_ipus)\n",
    "device_iterations = 32\n",
    "gradient_accumulation = 16 // replication_factor\n",
    "learning_rate = 1e-4\n",
    "num_epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf91184f",
   "metadata": {},
   "source": [
    "Create the `poptorch.Options` object with the right parameters setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4da15cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = poptorch.Options()\n",
    "options.enableExecutableCaching(executable_cache_dir)\n",
    "options.outputMode(poptorch.OutputMode.All)\n",
    "options.deviceIterations(device_iterations)\n",
    "options.replicationFactor(replication_factor)\n",
    "options.Training.gradientAccumulation(gradient_accumulation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ee43b6",
   "metadata": {},
   "source": [
    "We can also apply a few additional options that can help improve performance for SchNet.  These optimisations are covered in greater detail in [Extreme Acceleration of Graph Neural Network-based Prediction Models for Quantum Chemistry](https://arxiv.org/abs/2211.13853).  For the purpose of this notebook you can experiment with changing the `additional_optimizations` variable below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4d3610",
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_optimizations = True\n",
    "\n",
    "if additional_optimizations:\n",
    "    options = optimize_popart(options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e30f83",
   "metadata": {},
   "source": [
    "Recreate the dataloader with those options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43e2a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = CustomFixedSizeDataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_nodes=32 * (batch_size - 1),\n",
    "    options=options,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "65c2399c",
   "metadata": {},
   "source": [
    "Now if we inspect the first batch we will see the items are much larger than before. This is because PopTorch will split this batch between the selected number of device iterations, replicas and gradient accumulation steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5bbe94",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(next(iter(train_loader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c4680f",
   "metadata": {},
   "source": [
    "Create the SchNet model and pre-compile for the IPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1edce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "knn_graph = KNNInteractionGraph(cutoff=cutoff, k=28)\n",
    "model = SchNet(cutoff=cutoff, interaction_graph=knn_graph)\n",
    "model.train()\n",
    "model = TrainingModule(\n",
    "    model, batch_size=batch_size, replace_softplus=additional_optimizations\n",
    ")\n",
    "optimizer = poptorch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "training_model = poptorch.trainingModel(model, options, optimizer)\n",
    "\n",
    "data = next(iter(train_loader))\n",
    "training_model.compile(data.z, data.pos, data.batch, data.y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8dd792f",
   "metadata": {},
   "source": [
    "Train the model with the selected hyperparameters and log the mean loss from each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0530de",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    bar = tqdm(train_loader)\n",
    "    for i, data in enumerate(bar):\n",
    "        _, mini_batch_loss = training_model(data.z, data.pos, data.batch, data.y)\n",
    "        loss = float(mini_batch_loss.mean())\n",
    "        train.append({\"epoch\": epoch, \"step\": i, \"loss\": loss})\n",
    "        bar.set_description(f\"Epoch {epoch} loss: {loss:0.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c708be",
   "metadata": {},
   "source": [
    "Detach the training model from the IPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efc92ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_model.detachFromDevice()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e399928c",
   "metadata": {},
   "source": [
    "Plot the mean of the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12933e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(train)\n",
    "g = sns.lineplot(data=df[df.epoch > 0], x=\"epoch\", y=\"loss\", errorbar=\"sd\")\n",
    "g.set_xticks(range(0, num_epochs + 2, 2))\n",
    "g.figure.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "03c4127e",
   "metadata": {},
   "source": [
    "## Follow up\n",
    "\n",
    "The training loss looks like it is descreasing nicely over a relatively small number of epochs, try measuring the validation accuracy.  In the interest of time we haven't specified a large number of epochs, try increasing the epochs to achieve better accuracy.\n",
    "\n",
    "The following publications demonstrate using IPUs to train SchNet:\n",
    "\n",
    "* [Reducing Down(stream)time: Pretraining Molecular GNNs using Heterogeneous AI Accelerators](https://arxiv.org/abs/2211.04598)\n",
    "* [Extreme Acceleration of Graph Neural Network-based Prediction Models for Quantum Chemistry](https://arxiv.org/abs/2211.13853)\n",
    "\n",
    "The dataset used in these papers is available in PyG as [HydroNet](https://pytorch-geometric.readthedocs.io/en/stable/modules/datasets.html#torch_geometric.datasets.HydroNet)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.2.0+1262_poptorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (default, Nov 14 2022, 12:59:47) \n[GCC 9.4.0]"
  },
  "traceability": {
   "sdk_version": "3.2.0-EA.1+1249",
   "source_file": "molecular_property_prediction_with_schnet.py",
   "sst_version": "0.0.10",
   "timestamp": "2023-02-27T17:47"
  },
  "vscode": {
   "interpreter": {
    "hash": "4b7133ac454a965c7b25bf17fb22ff5a4b1e3a6d812b3e17a861b05f76606f09"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
