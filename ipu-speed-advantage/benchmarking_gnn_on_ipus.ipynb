{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ec425747",
   "metadata": {},
   "source": [
    "Copyright (c) 2023 Graphcore Ltd. All rights reserved."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e70cc6e3",
   "metadata": {},
   "source": [
    "# Benchmarking GNN message passing on IPU with PyTorch Geometric\n",
    "\n",
    "One of the key features of GNNs is message passing where a node's features are passed to its neighbours through specific operations. Message passing essentially consists of two parts, namely gathering the node embeddings onto their outbound edges and scattering the result onto the nodes that these edges connect to. There can be a number of additional operations between the gather and scatter, depending on the architecture of the model you are interested in, and therefore it is useful to consider the gather and scatter operations as distinct steps in message passing.\n",
    "\n",
    "![Message Passing](./static/gather_scatter.jpg \"Message Passing\")\n",
    "\n",
    "In this notebook we will take a look at the performance of the gather and scatter operations on Graphcore hardware in order to understand the performance advantage IPUs offer for these operations. Accelerating these fundamental operations with the IPU also directly translates into performance advantages in message passing and GNNs as a whole.\n",
    "\n",
    "In this notebook, we will:\n",
    " * Understand how to construct a harness to run benchmarks of particular operations on the IPU,\n",
    " * Run a sweep of scatter and gather operations on the IPU,\n",
    " * Take a look at the results of a larger sweep.\n",
    "\n",
    "Let's get started!\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4a466dc3",
   "metadata": {},
   "source": [
    "### Running on Paperspace\n",
    "\n",
    "The Paperspace environment lets you run this notebook with minimal effort. To improve your experience we preload datasets and preinstall packages, this can take a few minutes. If you experience errors immediately after starting a session, please try restarting the kernel before contacting support. If a problem persists or you want to give us feedback on the content of this notebook, please reach out to us through our community of developers using our [Slack channel](https://www.graphcore.ai/join-community) or raise a [GitHub issue](https://github.com/graphcore/examples)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "044362c3",
   "metadata": {},
   "source": [
    "\n",
    "In order to improve usability and support for future users, Graphcore would like to collect information about the\n",
    "applications and code being run in this notebook. The following information will be anonymised before being sent to Graphcore:\n",
    "\n",
    "- User progression through the notebook\n",
    "- Notebook details: number of cells, code being run and the output of the cells\n",
    "- Environment details\n",
    "\n",
    "You can disable logging at any time by running `%unload_ext graphcore_cloud_tools.notebook_logging.gc_logger` from any cell.\n",
    "\n",
    "To set up the requirements for running the session, simply run the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d15f513",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt\n",
    "%load_ext graphcore_cloud_tools.notebook_logging.gc_logger"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e51b6334",
   "metadata": {},
   "source": [
    "And for compatibility with the Paperspace environment variables we will do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b6533f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import poptorch\n",
    "\n",
    "poptorch.setLogLevel(\"ERR\")\n",
    "executable_cache_dir = (\n",
    "    os.getenv(\"POPLAR_EXECUTABLE_CACHE_DIR\", \"/tmp/exe_cache/\") + \"/pyg-benchmark\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bc6f84ad",
   "metadata": {},
   "source": [
    "Now we are ready to benchmark the gather and scatter operations on the IPU."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "737edde2",
   "metadata": {},
   "source": [
    "## Setting up the benchmarks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9542d774",
   "metadata": {},
   "source": [
    "In this section we will discuss how we will run the benchmarks in this notebook on the IPU.\n",
    "\n",
    "First we create a simple module which runs a scatter operation. \n",
    "\n",
    "This module does the following:\n",
    " * Creates random input nodes and edge tensors of sizes `[num_inputs, num_features]` and `[num_outputs, num_inputs]` respectively,\n",
    " * Performs a `torch scatter` operation with the requested reduce type, using these inputs and returns the output.\n",
    " * Registers `buffers` for the inputs and outputs, which reduces the host-IPU communication.\n",
    "\n",
    "See `benchmark_util.py` for the full implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57949b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from benchmark_util import ScatterOp\n",
    "\n",
    "op = ScatterOp(num_inputs=4, num_features=4, num_outputs=16, reduce=\"sum\")\n",
    "\n",
    "print(f\"{op.input = }\")\n",
    "print(f\"{op.index = }\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b11a447d",
   "metadata": {},
   "source": [
    "In the context of GNNs you can think of the scatter operation as sending the node features (which have been collected from the source nodes using the gather operation) to the target nodes of each edge, and using the reduce operation where the same target node receives multiple features. In this way, the input size corresponds to the number of outbound edges times the feature vector size for each node in the graph, while the size of the indices corresponds to the number of neighbours for each node (with the actual values being up to the total number of nodes).\n",
    "\n",
    "Now we wrap this module in a PopTorch `for_loop`. This creates a loop on the IPU that runs the benchmarked operation a specified number of times. We have put this in a separate module (`Benchmark`) which you can see in detail in `benchmark_util.py`.\n",
    "\n",
    "If you are unfamiliar with PopTorch, you can get started with running your models on the IPU by following our [Introduction to PopTorch tutorial](https://github.com/graphcore/examples/tree/v3.2.0/tutorials/tutorials/pytorch/basics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4cbbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from benchmark_util import Benchmark\n",
    "\n",
    "model = Benchmark(num_repeats=10, operator=op)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6fc9b0d9",
   "metadata": {},
   "source": [
    "This model is almost ready to run on IPUs but we still need to specify some PopTorch options we will use with the model. The most interesting ones are:\n",
    " * Turning on synthetic data - this disables host I/O to ensure we are measuring the performance of the operation only. See the [PopTorch documentation](https://docs.graphcore.ai/projects/poptorch-user-guide/en/latest/reference.html?highlight=enablesyntheticdata#poptorch.Options.enableSyntheticData) for more details. Feel free to try turning this option off and see how it changes the performance results.\n",
    " * Logging cycle count - this allows us to see the cycles of the operation. See the [PopTorch documentation](https://docs.graphcore.ai/projects/poptorch-user-guide/en/latest/reference.html?highlight=cycle%20count#poptorch.Options.logCycleCount) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6030516c",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = poptorch.Options()\n",
    "options.enableSyntheticData(True)\n",
    "options.logCycleCount(True)\n",
    "options.enableExecutableCaching(executable_cache_dir)\n",
    "options.connectionType(poptorch.ConnectionType.OnDemand)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5c56d9ef",
   "metadata": {},
   "source": [
    "We are now ready to convert our model into a PopTorch model and compile it, making it ready to run on IPUs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc35e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_model = poptorch.inferenceModel(model, options=options)\n",
    "pop_model.compile()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ff35ea3f",
   "metadata": {},
   "source": [
    "We now create a `PerfMetrics` object to log the performance of our model. This tracks the number of IPU cycles for each call and will give us an indication of bandwidth of the underlying operation. For full details see the implementation in `benchmark_util.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db363751",
   "metadata": {},
   "outputs": [],
   "source": [
    "from benchmark_util import PerfMetrics\n",
    "\n",
    "metrics = PerfMetrics(pop_model, num_repeats=5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a069f61a",
   "metadata": {},
   "source": [
    "Now we are ready to run a benchmark. We start by running a few warm up iterations to reduce measurement noise. Then we run the performance metrics in a loop and collect the benchmark results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2eef96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(pop_model, num_repeats):\n",
    "    num_warmup_rounds = 5\n",
    "\n",
    "    for _ in range(num_warmup_rounds):\n",
    "        _ = pop_model()\n",
    "\n",
    "    metrics = PerfMetrics(pop_model, num_repeats=num_repeats)\n",
    "    benchmark_results = []\n",
    "\n",
    "    for _ in range(num_repeats):\n",
    "        _ = pop_model()\n",
    "        values = metrics.update(pop_model.cycleCount())\n",
    "        benchmark_results.append(values)\n",
    "\n",
    "    return benchmark_results\n",
    "\n",
    "\n",
    "benchmark_results = benchmark(pop_model, num_repeats=5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "71cd3567",
   "metadata": {},
   "source": [
    "And we can view the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16091dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "04299305",
   "metadata": {},
   "source": [
    "Note that the effective bandwidth reported is based on the number of cycles and the performance of a Graphcore Bow-M2000.\n",
    "\n",
    "We now have some performance data for our scatter operation. This is the approach we will use for a range of sizes of operations for both scatter and gather operations, and demonstrate the high performance of Graphcore IPUs while running these operations leading to message passing acceleration in GNNs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b4c3f78c",
   "metadata": {},
   "source": [
    "## Running a sweep of *scatter* operations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "16e84f61",
   "metadata": {},
   "source": [
    "We will apply the same approach that we used to run and benchmark performance of a single scatter operation to run a sweep across a range of data sizes. This will demonstrate the high performance of IPUs when running scatter operations.\n",
    "\n",
    "First, let's decide on the parameters for our sweep and the range of values for each parameter. To save time running this notebook, we will only select a small sweep range. Feel free to extend the range to gather more performance data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3cc3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs = [2**e for e in range(5, 7)]\n",
    "num_features = [2**e for e in range(5, 7)]\n",
    "num_outputs = [2**e for e in range(10, 11)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4034f2cb",
   "metadata": {},
   "source": [
    "Next, create a Cartesian product of these parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff834b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "sweep_params = product(num_inputs, num_features, num_outputs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "46422664",
   "metadata": {},
   "source": [
    "Finally, run everything we have discussed above using the parameter sweep, logging the results to a `pandas` dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68992b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dfs = []\n",
    "num_repeats = 128\n",
    "\n",
    "for params in sweep_params:\n",
    "    print(\n",
    "        f\"Benchmarking scatter with input size {params[0]}, \"\n",
    "        f\"feature size {params[1]}, output size {params[2]}\"\n",
    "    )\n",
    "\n",
    "    op = ScatterOp(\n",
    "        num_inputs=params[0],\n",
    "        num_features=params[1],\n",
    "        num_outputs=params[2],\n",
    "        reduce=\"sum\",\n",
    "    )\n",
    "\n",
    "    model = Benchmark(num_repeats=10, operator=op)\n",
    "    pop_model = poptorch.inferenceModel(model, options=options)\n",
    "    pop_model.compile()\n",
    "\n",
    "    benchmark_results = benchmark(pop_model, num_repeats=10)\n",
    "\n",
    "    df = pd.DataFrame(benchmark_results)\n",
    "    p = {\n",
    "        k: [v] * len(df)\n",
    "        for k, v in zip((\"num_inputs\", \"num_features\", \"num_outputs\"), params)\n",
    "    }\n",
    "    df = pd.concat([pd.DataFrame(p), df], axis=1)\n",
    "    dfs.append(df.mean().to_frame().T)\n",
    "\n",
    "scatter_dfs = pd.concat(dfs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8ebe1898",
   "metadata": {},
   "source": [
    "Now let's visualise the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbebafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme()\n",
    "\n",
    "sns.pairplot(\n",
    "    data=scatter_dfs,\n",
    "    x_vars=[\"num_inputs\", \"num_outputs\"],\n",
    "    y_vars=[\"effective bandwidth (GiB/s)\", \"time (μs)\"],\n",
    "    hue=\"num_features\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "56b6b919",
   "metadata": {},
   "source": [
    "As we have only run a small sweep in the interest of time, here is a plot for a larger sweep:\n",
    "\n",
    "![Scatter Benchmarks](./static/scatter_benchmarks.png \"Scatter Benchmarks\")\n",
    "\n",
    "These results show how well the IPU performs across a wide range of hyperparameters for the scatter operation. We can observe a significant decrease in computation time as sparsity increases (so for smaller input and output sizes), a situation in which GPUs provide negligible improvements ([Hosseini et al., 2022](https://arxiv.org/abs/2207.09955)).\n",
    "So what is the performance gain with an IPU compared with an enterprise-grade GPU for example? The plots below show the performance gains for the size spectrum in which typical GNNs operate, as a speedup factor over the NVIDIA A100 (for completeness, we used <a href=\"https://github.com/graphcore-research/hydronet-gnn/blob/main/experiments/kernel_a100.ipynb\" target=\"_blank\">this notebook</a> to collect GPU data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e798629",
   "metadata": {},
   "outputs": [],
   "source": [
    "from benchmark_util import quick_benchmarks_3d\n",
    "\n",
    "quick_benchmarks_3d(\"scatter_add\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "37482a90",
   "metadata": {},
   "source": [
    "The figures highlight the advantage offered by the Graphcore IPU compared to an NVIDIA A100 (1x, blue plane). On the left we plot the entire performance data we have collected previously which includes a range of different feature sizes for each input-output pair as a scatter plot, while the figure on the right shows the average speedup across all feature sizes. We observe statistically significant speedups towards the lower range of tensor sizes, while the GPU approaches the IPU's performance as sizes increase. It is important to note that in the case of sparse access (small scatter input sizes) the IPU achieves over 16 times the performance of a GPU under the same conditions.\n",
    "\n",
    "Ok, we have seen excellent scatter results. Now we can do the same with gather to visualise IPU performance on that operation as well."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5e69c45a",
   "metadata": {},
   "source": [
    "## Running a sweep of *gather* operations\n",
    "\n",
    "First we create a module wrapping the `index_select` operation. This is very similar to the scatter module but feel free to take a look at the entire implementation in `benchmark_util.py` for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a66f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from benchmark_util import GatherOp\n",
    "\n",
    "op = GatherOp(num_inputs=4, num_features=4, num_outputs=16)\n",
    "\n",
    "print(f\"{op.input = }\")\n",
    "print(f\"{op.index = }\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3e18f8bb",
   "metadata": {},
   "source": [
    "In the context of GNNs you can think of the gather operation as collecting the node features onto the connecting edges. And so the input size can be thought of as the number of nodes, the feature size the size of the node embeddings and the output size the number of edges.\n",
    "\n",
    "Then, in a similar way to scatter, we can benchmark a sweep of different sized inputs and outputs to this operation. Again we have used the reduced the sweep range in the interest of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85721aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sweep_params = product(num_inputs, num_features, num_outputs)\n",
    "dfs = []\n",
    "num_repeats = 128\n",
    "\n",
    "for params in sweep_params:\n",
    "    print(\n",
    "        f\"Benchmarking gather with input size {params[0]}, \"\n",
    "        f\"feature size {params[1]}, output size {params[2]}\"\n",
    "    )\n",
    "\n",
    "    op = GatherOp(num_inputs=params[0], num_features=params[1], num_outputs=params[2])\n",
    "\n",
    "    model = Benchmark(num_repeats=10, operator=op)\n",
    "    pop_model = poptorch.inferenceModel(model, options=options)\n",
    "    pop_model.compile()\n",
    "\n",
    "    benchmark_results = benchmark(pop_model, num_repeats=10)\n",
    "\n",
    "    df = pd.DataFrame(benchmark_results)\n",
    "    p = {\n",
    "        k: [v] * len(df)\n",
    "        for k, v in zip((\"num_inputs\", \"num_features\", \"num_outputs\"), params)\n",
    "    }\n",
    "    df = pd.concat([pd.DataFrame(p), df], axis=1)\n",
    "    dfs.append(df.mean().to_frame().T)\n",
    "\n",
    "gather_dfs = pd.concat(dfs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "96a30cf8",
   "metadata": {},
   "source": [
    "And again lets visualise the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc2dea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme()\n",
    "\n",
    "sns.pairplot(\n",
    "    data=gather_dfs,\n",
    "    x_vars=[\"num_inputs\", \"num_outputs\"],\n",
    "    y_vars=[\"effective bandwidth (GiB/s)\", \"time (μs)\"],\n",
    "    hue=\"num_features\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2d73dc1f",
   "metadata": {},
   "source": [
    "Here is a larger sweep we have done previously in order to visualise the results:\n",
    "\n",
    "![Gather Benchmarks](./static/gather_benchmarks.png \"Gather Benchmarks\")\n",
    "\n",
    "Similarly to the scatter operation seen previously, these results show that sparse access is notably fast on the IPU with high performance in varying the number of inputs and outputs which GPUs do not tend to leverage ([Hosseini et al., 2022](https://arxiv.org/abs/2207.09955)).\n",
    "\n",
    "Moreover, it is important to note here that these results are collected for one-dimensional gather, in order to report fair comparison results by maintaining a linear correspondance between memory accesses and the number of inputs and outputs, while multi-dimensional tensors could provide even more performance benefits compared to the GPU.\n",
    "\n",
    "Let's look in closer detail on what this means in comparison to an enterprise-grade GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d271d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from benchmark_util import quick_benchmarks_3d\n",
    "\n",
    "quick_benchmarks_3d(\"gather\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9db214fb",
   "metadata": {},
   "source": [
    "Et voilà! We again observe over 8x speedups compared to an A100 baseline (1x, blue plane), with significantly higher performance across a wide range of hyperparameters, including the typical ranges within which GNN computations reside. This further confirms the advantages of the IPU's unique hardware characteristics which translate into more efficient compute in the realm of message passing neural networks."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f33b54ab",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We have successfully benchmarked gather and scatter operations on the IPU and seen signficant advantages when compared with GPUs. We also discussed these operations' relevance in message passing and thus GNNs as a whole, which helps shine a light on the relevance of these benchmark results. Concretely we have:\n",
    "\n",
    "* Understood how to set up an operation benchmark on the IPU,\n",
    "* Run a sweep of gather and scatter benchmarks on the IPU with various input and output size,\n",
    "* Taken a look at the results and seen the great performance the IPU has doing gather and scatter operations.\n",
    "\n",
    "## What next?\n",
    "\n",
    "Now that you have seen the great performance IPUs offer for message passing, now why not get started writing your own PyTorch Geometric model for the IPU:\n",
    "\n",
    "* Use our notebook for benchmarking gather scatter operations on the GPU to gather performance numbers to compare to,\n",
    "* Take a look at some of our tutorials for getting started with PyTorch Geometric on the IPU, for example our `learning-pytorch-geometric-on-ipus/1_at_a_glance.ipynb` or our `learning-pytorch-geometric-on-ipus/2_a_worked_example.ipynb` tutorial,\n",
    "* Run some of our examples, maybe try a molecular property prediction notebook like our example using SchNet: `graph-prediction/schnet-molecular-property-prediction/schnet_training.ipynb`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "aca87ce8e58c2e1f4bfbca448d30fec50515264d85b8d647ac1c7de314fef739"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
