Graphcore-Pytorch-README_first:
  location: ../
  generated: true
  notebook:
    file: README_first.ipynb

tutorial-basics-walkthrough:
  location: ../learning-PyTorch-on-IPU/basics
  generated: true
  notebook:
    file: walkthrough.ipynb
    timeout: 1200
  requirements_file: requirements.txt


efficient_data_loading-walkthrough:
  location: ../learning-PyTorch-on-IPU/efficient_data_loading
  generated: true
  notebook:
    file: walkthrough.ipynb
    timeout: 1200


mixed_precision-walkthrough:
  location: ../learning-PyTorch-on-IPU/mixed_precision
  generated: true
  notebook:
    file: walkthrough.ipynb
    timeout: 1200
  requirements_file: requirements.txt


pipelining-walkthrough:
  location: ../learning-PyTorch-on-IPU/pipelining
  generated: true
  notebook:
    file: walkthrough.ipynb
    timeout: 1200
  requirements_file: requirements.txt


finetuning-bert:
  location: ../finetuning-bert
  generated: true
  notebook:
    file: Fine-tuning-BERT.ipynb
    timeout: 10000
  requirements_file: requirements.txt

tgn:
  location: ../temporal-graph-networks
  generated: true
  notebook:
    file: Train_TGN.ipynb
    timeout: 10000
  requirements_file: requirements.txt


distributed-kge:
  location: ../distributed-kge
  generated: true
  notebook:
    file: KgeModelling.ipynb
    timeout: 10000
  requirements_file: requirements.txt

# Removed pending model work
# gpt2-textgen:
#   location: ../gpt2-text-generation
#   generated: true
#   notebook:
#     file: gpt2_sentiment_analysis.ipynb
#     timeout: 10000
#   requirements_file: requirements.txt

# Disabled pending upload of the chest-xray dataset
# vit-model-training:
#   location: ../vit-model-training
#   generated: true
#   notebook:
#     file: walkthrough.ipynb
#     timeout: 10000
#   requirements_file: requirements.txt

# GNN team testing separately
get-started-PyG-SchNetGNN:
  location: ../schnet-graph-network
  generated: true
  notebook:
    file: PyG-SchNetGNN.ipynb
    timeout: 3600
  requirements_file: requirements-pyg.txt



# Useful references
useful-managing-ipu-resources:
  location: ../useful-tips
  generated: true
  notebook:
    file: managing_ipu_resources.ipynb
    timeout: 1000

# examples_repository_options: &examples_repository_options
#   repository:
#     origin: https://github.com/graphcore/examples.git
#     ref: v3.0.0
#
# pytorch_resnet50_train_gen_pod16:
#   <<: *examples_repository_options
#   location: ../vision/cnns/pytorch/train
#   requirements_file: ../requirements.txt
#   required_apt_packages: ../required_apt_packages.txt
#   data:
#     throughput:
#       regexp: 'throughput: *(.*?) samples\/sec'
#       skip: 1
#     accuracy:
#       reduction_type: "final"
#       regexp: 'accuracy: *(.*?) \%'
#     loss:
#       reduction_type: "final"
#       regexp: 'loss: *(\d*\.\d*)'
#   output:
#     - [samples/sec, "throughput"]
#     - [accuracy, "accuracy"]
#     - [loss, "loss"]
#   env:
#     POPLAR_ENGINE_OPTIONS: '{"opt.enableMultiAccessCopies":"false"}'
#     PYTORCH_CACHE_DIR: "./pt_cache/"
#   description: ResNet training on 16 Mk2 IPUs, real data.
#   cmd: >-
#     poprun
#       -vv
#       --num-instances=8
#       --num-replicas=16
#       --executable-cache-path=$PYTORCH_CACHE_DIR
#     python3 train.py
#       --config resnet50
#       --data generated
#       --epoch 2
#       --validation-mode none
#       --dataloader-worker 14
#       --dataloader-rebatch-size 256


# pytorch_bert_large_packed_pretrain_gen_pod16:
#   <<: *examples_repository_options
#   location: ../nlp/bert/pytorch
#   requirements_file: requirements.txt
#   required_apt_packages:
#     - libopenmpi-dev
#   # required_apt_packages: required_apt_packages.txt # does not exist in 2.6
#   data:
#     throughput:
#       regexp: 'throughput: *(.*?) samples/sec'
#     mlm_acc:
#       regexp: 'mlm_acc: *(.*?) \%'
#       reduction_type: "final"
#     nsp_acc:
#       regexp: 'nsp_acc: *(.*?) \%'
#       reduction_type: "final"
#     nsp_loss:
#       regexp: 'nsp_loss: *(\d*\.\d*)'
#       reduction_type: "final"
#     mlm_loss:
#       regexp: 'mlm_loss: *(\d*\.\d*)'
#       reduction_type: "final"
#     loss:
#       regexp: 'total loss: *(\d*\.\d*)'
#       reduction_type: "final"
#   output:
#     - [samples/sec, "throughput"]
#     - [loss, "loss"]
#   description: |
#     BERT Large pretraining phase 1 and 2 with real data on 16 IPUs
#     for performance testing.
#   parameters:
#     phase: 128,512
#   cmd: >-
#     python3 run_pretraining.py
#         --config pretrain_large_{phase}
#         --training-steps 10
#         --dataset generated
#         --disable-progress-bar
#          --packed-data
