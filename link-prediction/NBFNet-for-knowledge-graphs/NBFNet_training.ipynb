{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a09f4f5-e147-4024-9deb-1e37753d14f9",
   "metadata": {},
   "source": [
    "Copyright (c) 2023 Graphcore Ltd. All rights reserved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b4dca6-3257-4709-aac0-9ae6441216aa",
   "metadata": {},
   "source": [
    "# Training Neural Bellman-Ford networks for inductive Knowledge Graph link prediction on IPU\n",
    "\n",
    "[Neural Bellman-Ford networks (NBFNet)](https://arxiv.org/abs/2106.06935) is a model that generalises path-based reasoning models for predicting links in homogeneous and heterogeneous graphs. \n",
    "\n",
    "In this notebook we apply NBFNet to link prediction in the FB15k-237 Knowledge Graph with 14541 entities, 237 relation types and 272115 triples. In practice we insert reverse edges, for a total of 474 relation types and 544230 triples. \n",
    "\n",
    "Unlike many other Knowledge Graph Completion models, NBFNet can be *inductive*, i.e. it can generalise to entities that did not appear in the training data. To demonstrate the inductiveness we train the model on a small subset of the graph (4707 entities, 54406 triples) and perform inference on the complete FB15k-237 graph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6e0a01-ac4e-431e-bf13-d433658b75e2",
   "metadata": {},
   "source": [
    "This notebook assumes some familiarity with PopTorch as well as PyTorch Geometric (PyG). For additional resources please consult:\n",
    "* [PopTorch Documentation](https://docs.graphcore.ai/projects/poptorch-user-guide/en/latest/index.html),\n",
    "* [PopTorch Examples and Tutorials](https://docs.graphcore.ai/en/latest/examples.html#pytorch),\n",
    "* [PyTorch Geometric](https://pytorch-geometric.readthedocs.io/en/latest/),\n",
    "* [PopTorch Geometric Documentation](https://docs.graphcore.ai/projects/poptorch-geometric-user-guide/en/latest/index.html),\n",
    "\n",
    "Requirements:\n",
    "* A Poplar SDK environment enabled (see the Getting Started guide for your IPU system)\n",
    "* Python packages installed with `pip install -r requirements.txt`\n",
    "\n",
    "## Running on Paperspace\n",
    "The Paperspace environment lets you run this notebook with no set up. To improve your experience we preload datasets and pre-install packages, this can take a few minutes, if you experience errors immediately after starting a session please try restarting the kernel before contacting support. If a problem persists or you want to give us feedback on the content of this notebook, please reach out to through our community of developers using our slack channel or raise a GitHub issue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6ccc79-9839-4930-8ad3-ea67c05d1223",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Install and import relevant dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "622cd22c-178b-48df-a6b8-6a17cea06cd3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c10605e9-a1af-4a6b-affd-7f2b046c9ca7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import notebook_utils\n",
    "\n",
    "import poptorch\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch_geometric.datasets import RelLinkPredDataset\n",
    "\n",
    "from nbfnet import NBFNet\n",
    "import data as nbfnet_data\n",
    "\n",
    "import inference_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d840e00-805e-4efe-b6a9-c1c16e6fbb28",
   "metadata": {},
   "source": [
    "And for compatibility with the Paperspace environment variables we will do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9040c772-cb87-4b81-8e76-bbd429209bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "poptorch.setLogLevel(\"ERR\")\n",
    "executable_cache_dir = (\n",
    "    os.getenv(\"POPLAR_EXECUTABLE_CACHE_DIR\", \"/tmp/exe_cache/\") + \"/pyg-nbfnet\"\n",
    ")\n",
    "dataset_directory = os.getenv(\"DATASET_DIR\", \"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa8d5c6-9ef5-4625-86c7-01645abe4b42",
   "metadata": {},
   "source": [
    "## 2. Define model hyperparameters\n",
    "Here we define some model settings and hyperparameters:\n",
    "- BATCH_SIZE: The micro batch size (number of triples `(head, relation, tail)`) during training\n",
    "- NUM_NEGATIVES: The number of triples `(head, relation, false tail)` to contrast against each true triple\n",
    "- LEARNING_RATE\n",
    "- LATENT_DIM: The hidden dimension in the Message Passing Neural Network\n",
    "- NUM_LAYERS: The number of message passing layers\n",
    "- NEG_ADVERSARIAL_TEMP: The temperature of a softmax that weights negative samples based on their difficulty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27d2cf4e-fc68-41c0-8fee-bb2cfeaf5adf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 6\n",
    "NUM_NEGATIVES = 32\n",
    "LEARNING_RATE = 0.001\n",
    "LATENT_DIM = 64\n",
    "NUM_LAYERS = 6\n",
    "NEG_ADVERSARIAL_TEMP = 0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43317bab-c1cc-48c8-9228-afea2b8f50e1",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3. Create dataset and dataloader\n",
    "We now build a dataset for training and validation from the small `IndFB15k-237_v4` graph and a dataset for inference from the full `FB15k-237` graph. Then we create dataloader for training, validation and inference. These take care of batching that data, removing edges between head and tail entity (just in the training case, to make the training objective non-trivial) and sampling negative tails. For validation and test, all entities will be treated as potential tail nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35952997-6009-4d59-a13f-c0fef7dc60a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/kkteru/grail/master/data/fb237_v4_ind/train.txt\n",
      "Downloading https://raw.githubusercontent.com/kkteru/grail/master/data/fb237_v4_ind/test.txt\n",
      "Downloading https://raw.githubusercontent.com/kkteru/grail/master/data/fb237_v4/train.txt\n",
      "Downloading https://raw.githubusercontent.com/kkteru/grail/master/data/fb237_v4/valid.txt\n",
      "Downloading https://raw.githubusercontent.com/MichSchli/RelationPrediction/master/data/FB-Toutanova/relations.dict\n",
      "Processing...\n",
      "Done!\n",
      "Downloading https://raw.githubusercontent.com/MichSchli/RelationPrediction/master/data/FB-Toutanova/entities.dict\n",
      "Downloading https://raw.githubusercontent.com/MichSchli/RelationPrediction/master/data/FB-Toutanova/relations.dict\n",
      "Downloading https://raw.githubusercontent.com/MichSchli/RelationPrediction/master/data/FB-Toutanova/test.txt\n",
      "Downloading https://raw.githubusercontent.com/MichSchli/RelationPrediction/master/data/FB-Toutanova/train.txt\n",
      "Downloading https://raw.githubusercontent.com/MichSchli/RelationPrediction/master/data/FB-Toutanova/valid.txt\n",
      "Processing...\n",
      "Done!\n",
      "/usr/local/lib/python3.8/dist-packages/torch_geometric/data/in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "dataset_train = nbfnet_data.build_dataset(\n",
    "    name=\"IndFB15k-237\", path=dataset_directory, version=\"v4\"\n",
    ")\n",
    "dataset_inference = nbfnet_data.build_dataset(name=\"FB15k-237\", path=dataset_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "464504b4-2a19-4c65-8db7-1869b6bc5459",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch_geometric/data/in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "dataloader = dict(\n",
    "    train=nbfnet_data.DataWrapper(\n",
    "        nbfnet_data.NBFData(\n",
    "            data=dataset_train[0],\n",
    "            batch_size=BATCH_SIZE,\n",
    "            is_training=True,\n",
    "            num_relations=dataset_train.num_relations,\n",
    "            num_negatives=NUM_NEGATIVES,\n",
    "        )\n",
    "    ),\n",
    "    valid=nbfnet_data.DataWrapper(\n",
    "        nbfnet_data.NBFData(\n",
    "            data=dataset_train[1],\n",
    "            batch_size=1,\n",
    "            is_training=False,\n",
    "        )\n",
    "    ),\n",
    "    test=nbfnet_data.DataWrapper(\n",
    "        nbfnet_data.NBFData(\n",
    "            data=dataset_inference[2],\n",
    "            batch_size=1,\n",
    "            is_training=False,\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "\n",
    "num_relations = dataset_inference.num_relations + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69d12aa-9444-4bec-b971-cc350dcd01f6",
   "metadata": {},
   "source": [
    "## 4. Define the model\n",
    "We can now define the model and the optimiser using the hyperparameters that we have defined above. The model is cast to float16 for improved compute- and memory efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db1c2480-9a8c-4d43-94e7-a2e5f7f6fc73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = NBFNet(\n",
    "    input_dim=LATENT_DIM,\n",
    "    hidden_dims=[LATENT_DIM] * NUM_LAYERS,\n",
    "    message_fct=\"mult\",\n",
    "    aggregation_fct=\"sum\",\n",
    "    num_mlp_layers=2,\n",
    "    relation_learning=\"linear_query\",\n",
    "    adversarial_temperature=NEG_ADVERSARIAL_TEMP,\n",
    "    num_relations=num_relations,\n",
    ")\n",
    "\n",
    "model.half();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12cfd3a0-d41b-4666-81d0-9ef76a586c27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optim = poptorch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=LEARNING_RATE,\n",
    "    bias_correction=True,\n",
    "    weight_decay=0.0,\n",
    "    eps=1e-8,\n",
    "    betas=(0.9, 0.999),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c5dc76-2100-4f3a-bc4c-f1eb3dfc1344",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### The model defines a `poptorch.Block` for every layer as well as for the preprocessing and prediction step. We can now assign IPUs to every block for a pipelined (or sharded) execution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9d29860-6b2f-455e-b88b-07f249cc168d",
   "metadata": {},
   "outputs": [],
   "source": [
    "available_ipus = os.getenv(\"NUM_AVAILABLE_IPU\", 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a61cc60-dc46-4bd5-b106-cdf38f26eae7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline = {\n",
    "    \"preprocessing\": 0,\n",
    "    \"layer0\": 0,\n",
    "    \"layer1\": 1,\n",
    "    \"layer2\": 1,\n",
    "    \"layer3\": 2,\n",
    "    \"layer4\": 2,\n",
    "    \"layer5\": 3,\n",
    "    \"prediction\": 3,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c53911d6-f3a0-4ee5-a70a-4eea85c43636",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline_plan = [poptorch.Stage(k).ipu(v) for k, v in pipeline.items()]\n",
    "\n",
    "train_opts = poptorch.Options()\n",
    "train_opts.setExecutionStrategy(poptorch.PipelinedExecution(*pipeline_plan))\n",
    "train_opts.Training.gradientAccumulation(16 if available_ipus == 16 else 64)\n",
    "if available_ipus == 16:\n",
    "    train_opts.replicationFactor(4)\n",
    "\n",
    "test_opts = poptorch.Options()\n",
    "test_opts.setExecutionStrategy(poptorch.PipelinedExecution(*pipeline_plan))\n",
    "test_opts.deviceIterations(len(set(pipeline.values())));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8fe613-f4f4-4458-b9e0-a6d8e4985499",
   "metadata": {},
   "source": [
    "#### We wrap the dataloader into a `poptorch.DataLoader` and the model into a `poptorch.trainingModel` or `poptorch.inferenceModel`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec3f607c-5c1e-428e-8126-24e6080916b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for partition in [\"train\", \"valid\", \"test\"]:\n",
    "    dataloader[partition] = poptorch.DataLoader(\n",
    "        options=train_opts if partition == \"train\" else test_opts,\n",
    "        dataset=dataloader[partition],\n",
    "        batch_size=1,\n",
    "        collate_fn=nbfnet_data.custom_collate,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3cdd7991-00de-4e6d-8963-2154c3edc6c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_train = poptorch.trainingModel(model, options=train_opts, optimizer=optim)\n",
    "model_valid = poptorch.inferenceModel(model, options=test_opts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b44cac1-4b3d-4a43-8ad9-f71b046cf76f",
   "metadata": {},
   "source": [
    "## 5. Training the model\n",
    "Now we are ready to train the model. We run training for 5 epochs on the IndFB15k-237_v4 subgraph with interleaved validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8172c5c-6bbe-401c-962f-bc9256f5e9b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[15:12:07.084] [poptorch:cpp] [warning] [DISPATCHER] Type coerced from Long to Int for tensor id 42\n",
      "[15:12:07.085] [poptorch:cpp] [warning] [DISPATCHER] Type coerced from Long to Int for tensor id 43\n",
      "[15:12:07.086] [poptorch:cpp] [warning] [DISPATCHER] Type coerced from Long to Int for tensor id 44\n",
      "[15:12:07.086] [poptorch:cpp] [warning] [DISPATCHER] Type coerced from Long to Int for tensor id 45\n",
      "[15:12:07.091] [poptorch:cpp] [warning] [DISPATCHER] Type coerced from Long to Int for tensor id 78\n",
      "[15:12:07.092] [poptorch:cpp] [warning] [DISPATCHER] Tensor (ptr 0x84e4b80) type coerced from Long to Int\n",
      "[15:12:07.098] [poptorch:cpp] [warning] [DISPATCHER] Type coerced from Long to Int for tensor id 127\n",
      "[15:12:07.099] [poptorch:cpp] [warning] [DISPATCHER] Tensor (ptr 0xbe5aed0) type coerced from Long to Int\n",
      "[15:12:07.104] [poptorch:cpp] [warning] [DISPATCHER] Type coerced from Long to Int for tensor id 176\n",
      "[15:12:07.104] [poptorch:cpp] [warning] [DISPATCHER] Tensor (ptr 0x8721c60) type coerced from Long to Int\n",
      "[15:12:07.109] [poptorch:cpp] [warning] [DISPATCHER] Type coerced from Long to Int for tensor id 225\n",
      "[15:12:07.109] [poptorch:cpp] [warning] [DISPATCHER] Tensor (ptr 0x8689890) type coerced from Long to Int\n",
      "[15:12:07.114] [poptorch:cpp] [warning] [DISPATCHER] Type coerced from Long to Int for tensor id 274\n",
      "[15:12:07.115] [poptorch:cpp] [warning] ...repeated messages suppressed...\n",
      "[15:12:07.120] [poptorch:cpp] [warning] [DISPATCHER] Type coerced from Long to Int for tensor id 323\n",
      "[15:12:07.124] [poptorch::python] [warning] No device set in torch.tensor(): forcing to IPU\n",
      "[15:12:07.124] [poptorch:cpp] [warning] [DISPATCHER] Type coerced from Long to Int for tensor id 357\n",
      "[15:12:07.125] [poptorch:cpp] [warning] [DISPATCHER] Type coerced from Long to Int for tensor id 359\n",
      "[15:12:07.128] [poptorch:cpp] [warning] [DISPATCHER] Type coerced from Long to Int for tensor id 387\n",
      "Graph compilation: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [04:47<00:00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 finished, training loss 0.3945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nethome/danielj/examples-internal/.venv_1234/lib/python3.8/site-packages/torch/nn/modules/module.py:1432: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "[15:19:01.183] [poptorch:cpp] [warning] [DISPATCHER] Type coerced from Long to Int for tensor id 458\n",
      "[15:19:01.187] [poptorch:cpp] [warning] [DISPATCHER] Type coerced from Long to Int for tensor id 459\n",
      "[15:19:01.187] [poptorch:cpp] [warning] [DISPATCHER] Type coerced from Long to Int for tensor id 460\n",
      "[15:19:01.188] [poptorch:cpp] [warning] [DISPATCHER] Type coerced from Long to Int for tensor id 461\n",
      "[15:19:01.222] [poptorch:cpp] [warning] [DISPATCHER] Type coerced from Long to Int for tensor id 494\n",
      "[15:19:01.267] [poptorch:cpp] [warning] [DISPATCHER] Type coerced from Long to Int for tensor id 548\n",
      "[15:19:01.275] [poptorch:cpp] [warning] [DISPATCHER] Type coerced from Long to Int for tensor id 602\n",
      "[15:19:01.282] [poptorch:cpp] [warning] [DISPATCHER] Type coerced from Long to Int for tensor id 656\n",
      "[15:19:01.288] [poptorch:cpp] [warning] [DISPATCHER] Type coerced from Long to Int for tensor id 710\n",
      "[15:19:01.295] [poptorch:cpp] [warning] [DISPATCHER] Type coerced from Long to Int for tensor id 764\n",
      "[15:19:01.303] [poptorch::python] [warning] No device set in torch.tensor(): forcing to IPU\n",
      "[15:19:01.312] [poptorch:cpp] [warning] [DISPATCHER] Type coerced from Long to Int for tensor id 803\n",
      "[15:19:01.314] [poptorch:cpp] [warning] [DISPATCHER] Type coerced from Long to Int for tensor id 805\n",
      "[15:19:01.321] [poptorch:cpp] [warning] [DISPATCHER] Type coerced from Long to Int for tensor id 833\n",
      "Graph compilation: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:53<00:00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, validation MRR 0.3189\n",
      "Epoch 1 finished, training loss 0.3013\n",
      "Epoch 1, validation MRR 0.3297\n",
      "Epoch 2 finished, training loss 0.2831\n",
      "Epoch 2, validation MRR 0.3347\n",
      "Epoch 3 finished, training loss 0.2874\n",
      "Epoch 3, validation MRR 0.3362\n",
      "Epoch 4 finished, training loss 0.2595\n",
      "Epoch 4, validation MRR 0.3417\n"
     ]
    }
   ],
   "source": [
    "model_train.train()\n",
    "model_valid.eval()\n",
    "\n",
    "loss_per_epoch = []\n",
    "mrr_per_epoch = []\n",
    "for epoch in range(5):\n",
    "    total_loss = 0\n",
    "    total_count = 0\n",
    "    for batch in dataloader[\"train\"]:\n",
    "        loss, count = model_train(**batch)\n",
    "        loss, count = loss.mean(), count.sum()  # reduction across replicas\n",
    "        total_loss += float(loss) * count\n",
    "        total_count += count\n",
    "    loss_per_epoch.append(total_loss / total_count)\n",
    "    print(f\"Epoch {epoch} finished, training loss {total_loss / total_count:.4}\")\n",
    "\n",
    "    # Interleaved validation\n",
    "    mrr = 0\n",
    "    total_count = 0\n",
    "    model_train.detachFromDevice()\n",
    "    for batch in dataloader[\"valid\"]:\n",
    "        prediction, count, mask, _ = model_valid(**batch)\n",
    "        if isinstance(count, torch.Tensor):\n",
    "            count = count.sum()\n",
    "        prediction = prediction[mask]\n",
    "        true_score = prediction[:, 0:1]\n",
    "        rank = torch.sum(true_score <= prediction, dim=-1)\n",
    "        mrr += float(torch.sum(1 / rank))\n",
    "        total_count += count\n",
    "    model_valid.detachFromDevice()\n",
    "    mrr_per_epoch.append(mrr / total_count)\n",
    "    print(f\"Epoch {epoch}, validation MRR {mrr / total_count:.4}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf25049-234b-473c-92df-8492c8cba713",
   "metadata": {},
   "source": [
    "# 6. Inference\n",
    "Finally, we can use our trained model to perform inference on FB15k-237. We define a `Prediction` class for all tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ff1eae5-4f79-4061-b164-5c00be5bc01b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inference_opts = poptorch.Options()\n",
    "inference_opts.setExecutionStrategy(poptorch.ShardedExecution(*pipeline_plan))\n",
    "model_inference = poptorch.inferenceModel(model, options=inference_opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "325d54ac-4cef-4e8d-a276-27b41b5cb54a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred = inference_utils.Prediction(\n",
    "    dataset_inference[0], \"static/fb15k-237_entitymapping.txt\", \"data/FB15k-237/raw/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61445cf9-2575-409e-ad6c-8e6d4ba2aed5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Run inference examples\n",
    "Now it is time to test the model on the bigger FB15k-237 graph and make some predictions of the form `(head, relation, ?)`.\n",
    "We use a simple string comparisons to match input strings to graph entities and relations. `pred.entity_vocab` and `pred.relation_vocab` contain lists of all available entities and relations.\n",
    "\n",
    "Note that the FB15k-237 graph is relatively small and not only lacks edges (which could be inferred using a knowledge graph completion model like this one) but also entities.\n",
    "\n",
    "`pred.inference` returns a list of entities and respective scores. Tails that occur in the graph are marked with an asterisk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e6599f01-fdeb-46db-9472-03af82f31194",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[15:30:12.991] [poptorch:cpp] [warning] [DISPATCHER] Type coerced from Long to Int for tensor id 877\n",
      "[15:30:13.005] [poptorch:cpp] [warning] [DISPATCHER] Type coerced from Long to Int for tensor id 878\n",
      "[15:30:13.006] [poptorch:cpp] [warning] [DISPATCHER] Type coerced from Long to Int for tensor id 879\n",
      "[15:30:13.006] [poptorch:cpp] [warning] [DISPATCHER] Type coerced from Long to Int for tensor id 880\n",
      "[15:30:13.016] [poptorch:cpp] [warning] [DISPATCHER] Type coerced from Long to Int for tensor id 909\n",
      "[15:30:13.026] [poptorch:cpp] [warning] [DISPATCHER] Type coerced from Long to Int for tensor id 968\n",
      "[15:30:13.033] [poptorch:cpp] [warning] [DISPATCHER] Type coerced from Long to Int for tensor id 1027\n",
      "[15:30:13.041] [poptorch:cpp] [warning] [DISPATCHER] Type coerced from Long to Int for tensor id 1086\n",
      "[15:30:13.048] [poptorch:cpp] [warning] [DISPATCHER] Type coerced from Long to Int for tensor id 1145\n",
      "[15:30:13.056] [poptorch:cpp] [warning] [DISPATCHER] Type coerced from Long to Int for tensor id 1204\n",
      "[15:30:13.061] [poptorch::python] [warning] No device set in torch.tensor(): forcing to IPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closest match for query ('London', '/location/location/contains (31)'), (4695, 31)\n",
      "31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[15:30:13.062] [poptorch:cpp] [warning] [DISPATCHER] Type coerced from Long to Int for tensor id 1248\n",
      "[15:30:13.063] [poptorch:cpp] [warning] [DISPATCHER] Type coerced from Long to Int for tensor id 1250\n",
      "[15:30:13.068] [poptorch:cpp] [warning] [DISPATCHER] Type coerced from Long to Int for tensor id 1278\n",
      "Graph compilation: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [02:43<00:00]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Architectural Association School of Architecture 9665', 5.55),\n",
       " ('Central Saint Martins 5180', 5.164),\n",
       " ('Goldsmiths, University of London 12717 *', 4.99),\n",
       " ('Middlesex University 13581', 4.953),\n",
       " ('Chelsea College of Art and Design 1238 *', 4.914)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.inference(model_inference, \"London\", \"/location/location/contains\", top_k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195b7c89-632f-4cf4-b991-8bb826db22ab",
   "metadata": {},
   "source": [
    "## Interpret results\n",
    "Another advantage of the NBFNet model is its interpretability. By passing edge weights of `1.0` along all edges we can later compute the derivative of a prediction with respect to these weights and thus identify the paths that were most important for the prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "520e51d7-0d32-4e61-b938-29d57c622cdb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred.path_importance(model, head_id=4695, tail_id=5180, relation_id=31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "91e1dacd-5016-44fe-8d18-4a385c147aa9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred.inference(model_inference, \"Good Will Hunting\", \"genre\", top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "808b125e-591c-4e1b-ad3a-3f6768458b76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred.path_importance(model, head_id=12481, tail_id=1810, relation_id=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365cfd3a-fc93-42f1-8099-63ee810ee568",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "Using a subgraph of FB15k-237 we have trained an inductive link prediction model for knowledge graphs. This model has been used to infer missing connections in the full FB15k-237 graph and could demonstrate the applied reasoning by outputting the paths in the graph that were most relevant to a given prediction.\n",
    "\n",
    "As a next step you could try to speed up training by replicating the model four times on a POD-16 or train on a larger graph. this could be achieved by reducing the batch size or pipelining the model over more IPUs. \n",
    "\n",
    "Also note that other PyTorch Geometric examples on are available for IPU to solve node-level or graph-level tasks. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
