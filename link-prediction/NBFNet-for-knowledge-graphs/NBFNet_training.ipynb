{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a09f4f5-e147-4024-9deb-1e37753d14f9",
   "metadata": {},
   "source": [
    "Copyright (c) 2023 Graphcore Ltd. All rights reserved."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "73b4dca6-3257-4709-aac0-9ae6441216aa",
   "metadata": {},
   "source": [
    "# Training Neural Bellman-Ford networks (NBFnet) for inductive knowledge graph link prediction on IPUs\n",
    "\n",
    "<a href=\"https://arxiv.org/abs/2106.06935\" target=\"_blank\">Neural Bellman-Ford networks (NBFNet)</a> is a model that generalises path-based reasoning models for predicting links in homogeneous and heterogeneous graphs. \n",
    "\n",
    "In this notebook we use NBFNet for link prediction in the FB15k-237 knowledge graph with 14541 entities, 237 relation types and 272115 triples. However in practice we explicitly insert reverse edges, which brings us to a total of 474 relation types and 544230 triples.\n",
    "\n",
    "Unlike many other knowledge graph completion models, NBFNet can be *inductive*, in other words it can generalise to entities that do not appear in the training data. To demonstrate this inductive behaviour we train the model on a small subset of the graph (4707 entities, 54406 triples) and perform inference on the complete FB15k-237 graph.\n",
    "\n",
    "This notebook assumes some familiarity with PopTorch as well as PyTorch Geometric (PyG). For additional resources please consult:\n",
    "* [PopTorch Documentation](https://docs.graphcore.ai/projects/poptorch-user-guide/en/latest/index.html),\n",
    "* [PopTorch Examples and Tutorials](https://docs.graphcore.ai/en/latest/examples.html#pytorch),\n",
    "* [PyTorch Geometric](https://pytorch-geometric.readthedocs.io/en/latest/),\n",
    "* [PopTorch Geometric Documentation](https://docs.graphcore.ai/projects/poptorch-geometric-user-guide/en/latest/index.html),\n",
    "\n",
    "### Running on Paperspace\n",
    "\n",
    "The Paperspace environment lets you run this notebook with no set up. To improve your experience we preload datasets and pre-install packages, this can take a few minutes, if you experience errors immediately after starting a session please try restarting the kernel before contacting support. If a problem persists or you want to give us feedback on the content of this notebook, please reach out to through our community of developers using our [slack channel](https://www.graphcore.ai/join-community) or raise a [GitHub issue](https://github.com/graphcore/examples).\n",
    "\n",
    "Requirements:\n",
    "\n",
    "* Python packages installed with `pip install -r requirements.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622cd22c-178b-48df-a6b8-6a17cea06cd3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install -q -r requirements.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4b89d3b9",
   "metadata": {},
   "source": [
    "Let's import the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10605e9-a1af-4a6b-affd-7f2b046c9ca7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import notebook_utils\n",
    "\n",
    "import os\n",
    "import os.path as osp\n",
    "\n",
    "import poptorch\n",
    "import torch\n",
    "from torch_geometric.datasets import RelLinkPredDataset\n",
    "\n",
    "import data as nbfnet_data\n",
    "import inference_utils\n",
    "from nbfnet import NBFNet"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7d840e00-805e-4efe-b6a9-c1c16e6fbb28",
   "metadata": {},
   "source": [
    "For compatibility with the Paperspace environment variables we need to do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9040c772-cb87-4b81-8e76-bbd429209bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "poptorch.setLogLevel(\"ERR\")\n",
    "executable_cache_dir = (\n",
    "    os.getenv(\"POPLAR_EXECUTABLE_CACHE_DIR\", \"/tmp/exe_cache/\") + \"/pyg-nbfnet\"\n",
    ")\n",
    "dataset_directory = os.getenv(\"DATASETS_DIR\", \"data\")\n",
    "available_ipus = int(os.getenv(\"NUM_AVAILABLE_IPU\", \"4\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6be914e8",
   "metadata": {},
   "source": [
    "Now we are ready to start!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "faa8d5c6-9ef5-4625-86c7-01645abe4b42",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "First we will look at the steps required to train the model.\n",
    "\n",
    "### Defining the model hyperparameters\n",
    "\n",
    "Here we define some model settings and hyperparameters:\n",
    "- BATCH_SIZE: The micro batch size (number of triples `(head, relation, tail)`) during training\n",
    "- NUM_NEGATIVES: The number of triples `(head, relation, false_tail)` to contrast against each true triple\n",
    "- LEARNING_RATE\n",
    "- LATENT_DIM: The hidden dimension in the Message Passing Neural Network\n",
    "- NUM_LAYERS: The number of message passing layers\n",
    "- NEG_ADVERSARIAL_TEMP: The temperature of a softmax that weights negative samples based on their difficulty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d2cf4e-fc68-41c0-8fee-bb2cfeaf5adf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 6\n",
    "NUM_NEGATIVES = 32\n",
    "LEARNING_RATE = 0.001\n",
    "LATENT_DIM = 64\n",
    "NUM_LAYERS = 6\n",
    "NEG_ADVERSARIAL_TEMP = 0.7"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "43317bab-c1cc-48c8-9228-afea2b8f50e1",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Creating the dataset and dataloader\n",
    "\n",
    "Now we build a training and validation dataset from the small `IndFB15k-237_v4` graph and a test dataset from the full `FB15k-237` graph. Then we create a dataloader for training, validation and test. The dataloader does the following: batches data; removes edges between head and tail entities in the training dataset to make the training objective non-trivial; samples negative tails. For validation and test, all entities will be treated as potential tail nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35952997-6009-4d59-a13f-c0fef7dc60a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_train = nbfnet_data.build_dataset(\n",
    "    name=\"IndFB15k-237\", path=dataset_directory, version=\"v4\"\n",
    ")\n",
    "dataset_inference = nbfnet_data.build_dataset(name=\"FB15k-237\", path=dataset_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357c4565-43ce-43ed-b986-4c5443d714ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataloader = dict(\n",
    "    train=nbfnet_data.DataWrapper(\n",
    "        nbfnet_data.NBFData(\n",
    "            data=dataset_train[0],\n",
    "            batch_size=BATCH_SIZE,\n",
    "            is_training=True,\n",
    "            num_relations=dataset_train.num_relations,\n",
    "            num_negatives=NUM_NEGATIVES,\n",
    "        )\n",
    "    ),\n",
    "    valid=nbfnet_data.DataWrapper(\n",
    "        nbfnet_data.NBFData(\n",
    "            data=dataset_train[1],\n",
    "            batch_size=1,\n",
    "            is_training=False,\n",
    "        )\n",
    "    ),\n",
    "    test=nbfnet_data.DataWrapper(\n",
    "        nbfnet_data.NBFData(\n",
    "            data=dataset_inference[2],\n",
    "            batch_size=1,\n",
    "            is_training=False,\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "\n",
    "num_relations = dataset_inference.num_relations + 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e69d12aa-9444-4bec-b971-cc350dcd01f6",
   "metadata": {},
   "source": [
    "### Defining the model\n",
    "We can now define the model and the optimiser using the hyperparameters that we have defined above. The model is cast to float16 for improved compute- and memory efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1c2480-9a8c-4d43-94e7-a2e5f7f6fc73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = NBFNet(\n",
    "    input_dim=LATENT_DIM,\n",
    "    hidden_dims=[LATENT_DIM] * NUM_LAYERS,\n",
    "    message_fct=\"mult\",\n",
    "    aggregation_fct=\"sum\",\n",
    "    num_mlp_layers=2,\n",
    "    relation_learning=\"linear_query\",\n",
    "    adversarial_temperature=NEG_ADVERSARIAL_TEMP,\n",
    "    num_relations=num_relations,\n",
    ")\n",
    "\n",
    "model.half();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cfd3a0-d41b-4666-81d0-9ef76a586c27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optim = poptorch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=LEARNING_RATE,\n",
    "    bias_correction=True,\n",
    "    weight_decay=0.0,\n",
    "    eps=1e-8,\n",
    "    betas=(0.9, 0.999),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a2c5dc76-2100-4f3a-bc4c-f1eb3dfc1344",
   "metadata": {
    "tags": []
   },
   "source": [
    "The model defines a `poptorch.Stage` for every layer as well as for the preprocessing and prediction step. We can now define the IPUs to every block for a pipelined (or sharded) execution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a61cc60-dc46-4bd5-b106-cdf38f26eae7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline = {\n",
    "    \"preprocessing\": 0,\n",
    "    \"layer0\": 0,\n",
    "    \"layer1\": 1,\n",
    "    \"layer2\": 1,\n",
    "    \"layer3\": 2,\n",
    "    \"layer4\": 2,\n",
    "    \"layer5\": 3,\n",
    "    \"prediction\": 3,\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7891a1b3",
   "metadata": {},
   "source": [
    "And assign them using the PopTorch options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53911d6-f3a0-4ee5-a70a-4eea85c43636",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline_plan = [poptorch.Stage(k).ipu(v) for k, v in pipeline.items()]\n",
    "\n",
    "train_opts = poptorch.Options()\n",
    "train_opts.setExecutionStrategy(poptorch.PipelinedExecution(*pipeline_plan))\n",
    "train_opts.Training.gradientAccumulation(16 if available_ipus == 16 else 64)\n",
    "if available_ipus == 16:\n",
    "    train_opts.replicationFactor(4)\n",
    "train_opts.enableExecutableCaching(executable_cache_dir)\n",
    "\n",
    "test_opts = poptorch.Options()\n",
    "test_opts.setExecutionStrategy(poptorch.PipelinedExecution(*pipeline_plan))\n",
    "test_opts.deviceIterations(len(set(pipeline.values())))\n",
    "test_opts.enableExecutableCaching(executable_cache_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dd8fe613-f4f4-4458-b9e0-a6d8e4985499",
   "metadata": {},
   "source": [
    "We wrap the dataloader into a `poptorch.DataLoader`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3f607c-5c1e-428e-8126-24e6080916b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for partition in [\"train\", \"valid\", \"test\"]:\n",
    "    dataloader[partition] = poptorch.DataLoader(\n",
    "        options=train_opts if partition == \"train\" else test_opts,\n",
    "        dataset=dataloader[partition],\n",
    "        batch_size=1,\n",
    "        collate_fn=nbfnet_data.custom_collate,\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e0abbed9",
   "metadata": {},
   "source": [
    "And wrap the model into `poptorch.trainingModel` or `poptorch.inferenceModel`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdd7991-00de-4e6d-8963-2154c3edc6c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_train = poptorch.trainingModel(model, options=train_opts, optimizer=optim)\n",
    "model_valid = poptorch.inferenceModel(model, options=test_opts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f27be028",
   "metadata": {},
   "source": [
    "Now we are ready to start training."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8b44cac1-4b3d-4a43-8ad9-f71b046cf76f",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "Now we are ready to train the model. We run training for 5 epochs on the IndFB15k-237_v4 subgraph with interleaved validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735d8e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8172c5c-6bbe-401c-962f-bc9256f5e9b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_train.train()\n",
    "model_valid.eval()\n",
    "\n",
    "loss_per_epoch = []\n",
    "mrr_per_epoch = []\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    total_count = 0\n",
    "    for batch in dataloader[\"train\"]:\n",
    "        loss, count = model_train(**batch)\n",
    "        loss, count = loss.mean(), count.sum()  # reduction across replicas\n",
    "        total_loss += float(loss) * count\n",
    "        total_count += count\n",
    "    loss_per_epoch.append(total_loss / total_count)\n",
    "    print(f\"Epoch {epoch} finished, training loss {total_loss / total_count:.4}\")\n",
    "\n",
    "    # Interleaved validation\n",
    "    mrr = 0\n",
    "    total_count = 0\n",
    "    model_train.detachFromDevice()\n",
    "    for batch in dataloader[\"valid\"]:\n",
    "        prediction, count, mask, _ = model_valid(**batch)\n",
    "        if isinstance(count, torch.Tensor):\n",
    "            count = count.sum()\n",
    "        prediction = prediction[mask]\n",
    "        true_score = prediction[:, 0:1]\n",
    "        rank = torch.sum(true_score <= prediction, dim=-1)\n",
    "        mrr += float(torch.sum(1 / rank))\n",
    "        total_count += count\n",
    "    model_valid.detachFromDevice()\n",
    "    mrr_per_epoch.append(mrr / total_count)\n",
    "    print(f\"Epoch {epoch}, validation MRR {mrr / total_count:.4}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bdf25049-234b-473c-92df-8492c8cba713",
   "metadata": {},
   "source": [
    "### Running inference on the trained model\n",
    "\n",
    "Finally, we can use our trained model to perform inference on FB15k-237."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff1eae5-4f79-4061-b164-5c00be5bc01b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inference_opts = poptorch.Options()\n",
    "inference_opts.setExecutionStrategy(poptorch.ShardedExecution(*pipeline_plan))\n",
    "inference_opts.enableExecutableCaching(executable_cache_dir)\n",
    "model_inference = poptorch.inferenceModel(model, options=inference_opts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2ad61176",
   "metadata": {},
   "source": [
    "We wrap the detaset with a `Prediction` object to simplify the inference process for all the different tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325d54ac-4cef-4e8d-a276-27b41b5cb54a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred = inference_utils.Prediction(\n",
    "    dataset_inference[0],\n",
    "    \"static/fb15k-237_entitymapping.txt\",\n",
    "    osp.join(dataset_directory, \"FB15k-237/raw/\"),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c7d3a99a",
   "metadata": {},
   "source": [
    "And run inference with this class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d242f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.inference(model_inference, \"Good Will Hunting\", \"genre\", top_k=5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "61445cf9-2575-409e-ad6c-8e6d4ba2aed5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Running inference on the FB15k-237 graph\n",
    "\n",
    "Now it is time to test the model on the bigger FB15k-237 graph and make some predictions of the form `(head, relation, ?)`.\n",
    "We use a simple string comparison to match input strings to graph entities and relations. `pred.entity_vocab` and `pred.relation_vocab` contain lists of all available entities and relations.\n",
    "\n",
    "Note that the FB15k-237 graph is relatively small and not only lacks edges (which could be inferred using a knowledge graph completion model like this one) but also entities.\n",
    "\n",
    "`pred.inference` returns a list of entities and respective scores. Tails that occur in the graph are marked with an asterisk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6599f01-fdeb-46db-9472-03af82f31194",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred.inference(model_inference, \"London\", \"/location/location/contains\", top_k=5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "195b7c89-632f-4cf4-b991-8bb826db22ab",
   "metadata": {},
   "source": [
    "## Interpreting the results\n",
    "\n",
    "Another advantage of the NBFNet model is its interpretability. By passing edge weights of `1.0` along all edges we can later compute the derivative of a prediction with respect to these weights and thus identify the paths that were most important for the prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520e51d7-0d32-4e61-b938-29d57c622cdb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred.path_importance(model, head_id=4695, tail_id=5180, relation_id=31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808b125e-591c-4e1b-ad3a-3f6768458b76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred.path_importance(model, head_id=12481, tail_id=1810, relation_id=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "365cfd3a-fc93-42f1-8099-63ee810ee568",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Using a subgraph of FB15k-237 we have trained an inductive link prediction model for knowledge graphs. This model has been used to infer missing connections in the full FB15k-237 graph and could demonstrate the applied reasoning by outputting the paths in the graph that were most relevant to a given prediction.\n",
    "\n",
    "As a next step you could try to speed up training by replicating the model four times on a POD-16 or train on a larger graph. this could be achieved by reducing the batch size or pipelining the model over more IPUs. \n",
    "\n",
    "If you are interested in node-level or graph-level tasks, take a look at our other examples. For instance, [Prediction of Molecular Properties using SchNet on Graphcore IPUs](../../schnet/pytorch_geometric/molecular_property_prediction_with_schnet.ipynb) for graph prediction or [Cluster GCN on IPU: Node classification task on a large graph using sampling](../../cluster_gcn/pytorch_geometric/node_classification_with_cluster_gcn.ipynb) for node prediction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.2.0+1275_poptorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (default, Nov 14 2022, 12:59:47) \n[GCC 9.4.0]"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "vscode": {
   "interpreter": {
    "hash": "d0f97823f57b0f089f60f21c28af4cd6be09805a02eb8190e59c2569211fe3cd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
